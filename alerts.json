{
  "status": "success",
  "data": {
    "groups": [
      {
        "name": "CloudCredentialOperator",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cloud-credential-operator-cloud-credential-operator-alerts.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "CloudCredentialOperatorTargetNamespaceMissing",
            "query": "cco_credentials_requests_conditions{condition=\"MissingTargetNamespace\"} > 0",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "CredentialsRequest(s) pointing to non-existant namespace"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000331632,
            "lastEvaluation": "2023-09-21T23:03:42.8525011Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CloudCredentialOperatorProvisioningFailed",
            "query": "cco_credentials_requests_conditions{condition=\"CredentialsProvisionFailure\"} > 0",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "CredentialsRequest(s) unable to be fulfilled"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000253804,
            "lastEvaluation": "2023-09-21T23:03:42.852834706Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CloudCredentialOperatorDeprovisioningFailed",
            "query": "cco_credentials_requests_conditions{condition=\"CredentialsDeprovisionFailure\"} > 0",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "CredentialsRequest(s) unable to be cleaned up"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00017809,
            "lastEvaluation": "2023-09-21T23:03:42.853092389Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CloudCredentialOperatorInsufficientCloudCreds",
            "query": "cco_credentials_requests_conditions{condition=\"InsufficientCloudCreds\"} > 0",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster's cloud credentials insufficient for minting or passthrough"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000151616,
            "lastEvaluation": "2023-09-21T23:03:42.853271507Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000934335,
        "lastEvaluation": "2023-09-21T23:03:42.852492533Z"
      },
      {
        "name": "cluster-machine-approver.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-machine-approver-machineapprover-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ClusterMachineApproverDown",
            "query": "absent(up{job=\"machine-approver\"} == 1)",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "ClusterMachineApprover has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000249967,
            "lastEvaluation": "2023-09-21T23:04:00.185604667Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "MachineApproverMaxPendingCSRsReached",
            "query": "mapi_current_pending_csr > mapi_max_pending_csr",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "max pending CSRs threshold reached."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000182547,
            "lastEvaluation": "2023-09-21T23:04:00.185856797Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000444083,
        "lastEvaluation": "2023-09-21T23:04:00.185600751Z"
      },
      {
        "name": "SamplesOperator",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-samples-operator-samples-operator-alerts.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "SamplesRetriesMissingOnImagestreamImportFailing",
            "query": "sum(openshift_samples_failed_imagestream_import_info) > sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m)",
            "duration": 7200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Samples operator is detecting problems with imagestream image imports, and the periodic retries of those\nimports are not occurring.  Contact support.  You can look at the \"openshift-samples\" ClusterOperator object\nfor details. Most likely there are issues with the external image registry hosting the images that need to\nbe investigated.  The list of ImageStreams that have failing imports are:\n{{ range query \"openshift_samples_failed_imagestream_import_info > 0\" }}\n  {{ .Labels.name }}\n{{ end }}\nHowever, the list of ImageStreams for which samples operator is retrying imports is:\nretrying imports:\n{{ range query \"openshift_samples_retry_imagestream_import_total > 0\" }}\n   {{ .Labels.imagestreamname }}\n{{ end }}\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001836667,
            "lastEvaluation": "2023-09-21T23:04:07.343284089Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SamplesImagestreamImportFailing",
            "query": "sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m) > sum(openshift_samples_failed_imagestream_import_info)",
            "duration": 7200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Samples operator is detecting problems with imagestream image imports.  You can look at the \"openshift-samples\"\nClusterOperator object for details. Most likely there are issues with the external image registry hosting\nthe images that needs to be investigated.  Or you can consider marking samples opertaor Removed if you do not\ncare about having sample imagestreams available.  The list of ImageStreams for which samples operator is\nretrying imports:\n{{ range query \"openshift_samples_retry_imagestream_import_total > 0\" }}\n   {{ .Labels.imagestreamname }}\n{{ end }}\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001565065,
            "lastEvaluation": "2023-09-21T23:04:07.345124813Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SamplesDegraded",
            "query": "openshift_samples_degraded_info == 1",
            "duration": 7200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Samples could not be deployed and the operator is degraded. Review the \"openshift-samples\" ClusterOperator object for further details.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000284669,
            "lastEvaluation": "2023-09-21T23:04:07.346693158Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SamplesInvalidConfig",
            "query": "openshift_samples_invalidconfig_info == 1",
            "duration": 7200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Samples operator has been given an invalid configuration.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000182551,
            "lastEvaluation": "2023-09-21T23:04:07.346981297Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SamplesMissingSecret",
            "query": "openshift_samples_invalidsecret_info{reason=\"missing_secret\"} == 1",
            "duration": 7200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Samples operator cannot find the samples pull secret in the openshift namespace.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000191874,
            "lastEvaluation": "2023-09-21T23:04:07.347165515Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SamplesMissingTBRCredential",
            "query": "openshift_samples_invalidsecret_info{reason=\"missing_tbr_credential\"} == 1",
            "duration": 7200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000166387,
            "lastEvaluation": "2023-09-21T23:04:07.347358928Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SamplesTBRInaccessibleOnBoot",
            "query": "openshift_samples_tbr_inaccessible_info == 1",
            "duration": 172800,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "message": "Samples operator could not access 'registry.redhat.io' during its initial installation and it boostrapped as removed.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000182323,
            "lastEvaluation": "2023-09-21T23:04:07.347527073Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.004439519,
        "lastEvaluation": "2023-09-21T23:04:07.343276248Z"
      },
      {
        "name": "cluster-operators",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-version-cluster-version-operator.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ClusterNotUpgradeable",
            "query": "max by(name, condition, endpoint) (cluster_operator_conditions{condition=\"Upgradeable\",endpoint=\"metrics\",name=\"version\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "One or more cluster operators have been blocking minor version cluster upgrades for at least an hour for reason {{ with $cluster_operator_conditions := \"cluster_operator_conditions\" | query}}{{range $value := .}}{{if and (eq (label \"name\" $value) \"version\") (eq (label \"condition\" $value) \"Upgradeable\") (eq (label \"endpoint\" $value) \"metrics\") (eq (value $value) 0.0) (ne (len (label \"reason\" $value)) 0) }}{{label \"reason\" $value}}.{{end}}{{end}}{{end}} {{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} For more information refer to {{ label \"url\" (first $console_url ) }}/settings/cluster/.{{ end }}{{ end }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000349677,
            "lastEvaluation": "2023-09-21T23:03:59.213465878Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "ClusterOperatorDown",
            "query": "cluster_operator_up{job=\"cluster-version-operator\"} == 0",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Cluster operator {{ $labels.name }} has not been available for 10 minutes. Operator may be down or disabled, cluster will not be kept up to date and upgrades will not be possible."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "ClusterOperatorDown",
                  "endpoint": "metrics",
                  "instance": "192.168.50.10:9099",
                  "job": "cluster-version-operator",
                  "name": "monitoring",
                  "namespace": "openshift-cluster-version",
                  "pod": "cluster-version-operator-56889ff669-vqrpb",
                  "service": "cluster-version-operator",
                  "severity": "critical",
                  "version": "4.6.36"
                },
                "annotations": {
                  "message": "Cluster operator monitoring has not been available for 10 minutes. Operator may be down or disabled, cluster will not be kept up to date and upgrades will not be possible."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:29.21303266Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001191424,
            "lastEvaluation": "2023-09-21T23:03:59.213818207Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "ClusterOperatorDegraded",
            "query": "cluster_operator_conditions{condition=\"Degraded\",job=\"cluster-version-operator\"} == 1",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Cluster operator {{ $labels.name }} has been degraded for 10 minutes. Operator is degraded because {{ $labels.reason }} and cluster upgrades will be unstable."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "ClusterOperatorDegraded",
                  "condition": "Degraded",
                  "endpoint": "metrics",
                  "instance": "192.168.50.10:9099",
                  "job": "cluster-version-operator",
                  "name": "monitoring",
                  "namespace": "openshift-cluster-version",
                  "pod": "cluster-version-operator-56889ff669-vqrpb",
                  "reason": "UpdatingAlertmanagerFailed",
                  "service": "cluster-version-operator",
                  "severity": "critical"
                },
                "annotations": {
                  "message": "Cluster operator monitoring has been degraded for 10 minutes. Operator is degraded because UpdatingAlertmanagerFailed and cluster upgrades will be unstable."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:13:29.21303266Z",
                "value": "1e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001316513,
            "lastEvaluation": "2023-09-21T23:03:59.215013031Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ClusterOperatorFlapping",
            "query": "changes(cluster_operator_up{job=\"cluster-version-operator\"}[2m]) > 2",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster operator {{ $labels.name }} up status is changing often. This might cause upgrades to be unstable."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000930656,
            "lastEvaluation": "2023-09-21T23:03:59.216333424Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.003813464,
        "lastEvaluation": "2023-09-21T23:03:59.213455513Z"
      },
      {
        "name": "cluster-version",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-version-cluster-version-operator.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ClusterVersionOperatorDown",
            "query": "absent(up{job=\"cluster-version-operator\"} == 1)",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Cluster version operator has disappeared from Prometheus target discovery. Operator may be down or disabled, cluster will not be kept up to date and upgrades will not be possible."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000342078,
            "lastEvaluation": "2023-09-21T23:03:46.457249223Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CannotRetrieveUpdates",
            "query": "(time() - cluster_version_operator_update_retrieval_timestamp_seconds) >= 3600 and ignoring(condition, name, reason) cluster_operator_conditions{condition=\"RetrievedUpdates\",endpoint=\"metrics\",name=\"version\",reason!=\"NoChannel\"}",
            "duration": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Cluster version operator has not retrieved updates in {{ $value | humanizeDuration }}. Failure reason {{ with $cluster_operator_conditions := \"cluster_operator_conditions\" | query}}{{range $value := .}}{{if and (eq (label \"name\" $value) \"version\") (eq (label \"condition\" $value) \"RetrievedUpdates\") (eq (label \"endpoint\" $value) \"metrics\") (eq (value $value) 0.0)}}{{label \"reason\" $value}} {{end}}{{end}}{{end}}. {{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} For more information refer to {{ label \"url\" (first $console_url ) }}/settings/cluster/.{{ end }}{{ end }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000296552,
            "lastEvaluation": "2023-09-21T23:03:46.457592855Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "UpdateAvailable",
            "query": "cluster_version_available_updates > 0",
            "duration": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "message": "Your upstream update recommendation service recommends you update your cluster.  For more information refer to 'oc adm upgrade'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00014397,
            "lastEvaluation": "2023-09-21T23:03:46.457891308Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.00079515,
        "lastEvaluation": "2023-09-21T23:03:46.457243973Z"
      },
      {
        "name": "openshift-dns.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-dns-operator-dns.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "CoreDNSPanicking",
            "query": "increase(coredns_panic_count_total[10m]) > 0",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "{{ $value }} CoreDNS panics observed on {{ $labels.instance }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000595777,
            "lastEvaluation": "2023-09-21T23:03:55.019037745Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CoreDNSHealthCheckSlow",
            "query": "histogram_quantile(0.95, sum by(instance, le) (rate(coredns_health_request_duration_seconds_bucket[5m]))) > 10",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "CoreDNS Health Checks are slowing down (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.003212876,
            "lastEvaluation": "2023-09-21T23:03:55.019637926Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CoreDNSErrorsHigh",
            "query": "(sum(rate(coredns_dns_response_rcode_count_total{rcode=\"SERVFAIL\"}[5m])) / sum(rate(coredns_dns_response_rcode_count_total[5m]))) > 0.01",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000481163,
            "lastEvaluation": "2023-09-21T23:03:55.022854807Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.004307411,
        "lastEvaluation": "2023-09-21T23:03:55.019031743Z"
      },
      {
        "name": "ImageRegistryOperator",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-image-registry-image-registry-operator-alerts.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ImageRegistryStorageReconfigured",
            "query": "increase(image_registry_operator_storage_reconfigured_total[30m]) > 0",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Image Registry Storage configuration has changed in the last 30\nminutes. This change may have caused data loss.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00023992,
            "lastEvaluation": "2023-09-21T23:04:01.116120223Z",
            "type": "alerting"
          },
          {
            "name": ":apiserver_v1_image_imports:sum",
            "query": "sum(apiserver_v1_image_imports_total)",
            "health": "ok",
            "evaluationTime": 5.4322e-05,
            "lastEvaluation": "2023-09-21T23:04:01.116361358Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000305643,
        "lastEvaluation": "2023-09-21T23:04:01.11611306Z"
      },
      {
        "name": "openshift-ingress.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ingress-operator-ingress-operator.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "HAProxyReloadFail",
            "query": "template_router_reload_failure == 1",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting recently created or modified routes"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000331157,
            "lastEvaluation": "2023-09-21T23:03:51.97676697Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "HAProxyDown",
            "query": "haproxy_up == 0",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "HAProxy metrics are reporting that the router is down"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000157103,
            "lastEvaluation": "2023-09-21T23:03:51.977100006Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000501752,
        "lastEvaluation": "2023-09-21T23:03:51.976758889Z"
      },
      {
        "name": "apiserver-requests-in-flight",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver.yaml",
        "rules": [
          {
            "name": "cluster:apiserver_current_inflight_requests:sum:max_over_time:2m",
            "query": "max_over_time(sum by(apiserver, requestKind) (apiserver_current_inflight_requests{apiserver=~\"openshift-apiserver|kube-apiserver\"})[2m:])",
            "health": "ok",
            "evaluationTime": 0.003282546,
            "lastEvaluation": "2023-09-21T23:03:53.70623464Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.00329994,
        "lastEvaluation": "2023-09-21T23:03:53.70622484Z"
      },
      {
        "name": "cluster-version",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-operator-kube-apiserver-operator.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "TechPreviewNoUpgrade",
            "query": "cluster_feature_set{name!=\"\",namespace=\"openshift-kube-apiserver-operator\"} == 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster has enabled tech preview features that will prevent upgrades."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002669611,
            "lastEvaluation": "2023-09-21T23:04:04.141461035Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.002687105,
        "lastEvaluation": "2023-09-21T23:04:04.141449015Z"
      },
      {
        "name": "cluster-version",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-controller-manager-operator-kube-controller-manager-operator.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeControllerManagerDown",
            "query": "absent(up{job=\"kube-controller-manager\"} == 1)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "KubeControllerManager has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000491064,
            "lastEvaluation": "2023-09-21T23:03:46.084679655Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PodDisruptionBudgetAtLimit",
            "query": "max by(namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_expected_pods == kube_poddisruptionbudget_status_desired_healthy)",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "The pod disruption budget is preventing further disruption to pods because it is at the minimum allowed level."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000241561,
            "lastEvaluation": "2023-09-21T23:03:46.085173603Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PodDisruptionBudgetLimit",
            "query": "max by(namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_expected_pods < kube_poddisruptionbudget_status_desired_healthy)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "The pod disruption budget is below the minimum number allowed pods."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000175204,
            "lastEvaluation": "2023-09-21T23:03:46.085416647Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000925313,
        "lastEvaluation": "2023-09-21T23:03:46.084669744Z"
      },
      {
        "name": "cluster-version",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-scheduler-operator-kube-scheduler-operator.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeSchedulerDown",
            "query": "absent(up{job=\"scheduler\"} == 1)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "KubeScheduler has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00063348,
            "lastEvaluation": "2023-09-21T23:03:48.259312662Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000649113,
        "lastEvaluation": "2023-09-21T23:03:48.259303168Z"
      },
      {
        "name": "Cluster-autoscaler-operator-down",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-cluster-autoscaler-operator-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ClusterAutoscalerOperatorDown",
            "query": "absent(up{job=\"cluster-autoscaler-operator\"} == 1)",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "cluster-autoscaler-operator has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000629992,
            "lastEvaluation": "2023-09-21T23:03:41.801712631Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000648956,
        "lastEvaluation": "2023-09-21T23:03:41.801701997Z"
      },
      {
        "name": "machine-api-operator-down",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MachineAPIOperatorDown",
            "query": "absent(up{job=\"machine-api-operator\"} == 1)",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "machine api operator is down"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000596179,
            "lastEvaluation": "2023-09-21T23:04:01.59908426Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000611451,
        "lastEvaluation": "2023-09-21T23:04:01.599076423Z"
      },
      {
        "name": "machine-api-operator-metrics-collector-up",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MachineAPIOperatorMetricsCollectionFailing",
            "query": "mapi_mao_collector_up == 0",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "machine api operator metrics collection is failing. For more details:  oc logs <machine-api-operator-pod-name> -n openshift-machine-api"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000459304,
            "lastEvaluation": "2023-09-21T23:03:50.375318334Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000472952,
        "lastEvaluation": "2023-09-21T23:03:50.375308884Z"
      },
      {
        "name": "machine-with-no-running-phase",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MachineWithNoRunningPhase",
            "query": "(mapi_machine_created_timestamp_seconds{phase!=\"Running\"}) > 0",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "machine {{ $labels.name }} is in phase: {{ $labels.phase }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000478731,
            "lastEvaluation": "2023-09-21T23:03:55.237110242Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000491198,
        "lastEvaluation": "2023-09-21T23:03:55.237102686Z"
      },
      {
        "name": "machine-without-valid-node-ref",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MachineWithoutValidNode",
            "query": "(mapi_machine_created_timestamp_seconds unless on(node) kube_node_info) > 0",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "machine {{ $labels.name }} does not have valid node reference"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000755332,
            "lastEvaluation": "2023-09-21T23:03:44.125922508Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000772161,
        "lastEvaluation": "2023-09-21T23:03:44.125913641Z"
      },
      {
        "name": "mcd-drain-error",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MCDDrainError",
            "query": "mcd_drain_err > 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Drain failed on {{ $labels.node }} , updates may be blocked. For more details:  oc logs -f -n openshift-machine-config-operator machine-config-daemon-<hash> -c machine-config-daemon"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000338849,
            "lastEvaluation": "2023-09-21T23:04:06.021836245Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000357722,
        "lastEvaluation": "2023-09-21T23:04:06.021826407Z"
      },
      {
        "name": "mcd-kubelet-health-state-error",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeletHealthState",
            "query": "mcd_kubelet_state > 2",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Kubelet health failure threshold reached"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000206904,
            "lastEvaluation": "2023-09-21T23:03:52.348314516Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000213669,
        "lastEvaluation": "2023-09-21T23:03:52.34831143Z"
      },
      {
        "name": "mcd-pivot-error",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MCDPivotError",
            "query": "mcd_pivot_err > 0",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Error detected in pivot logs on {{ $labels.node }} "
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000292051,
            "lastEvaluation": "2023-09-21T23:04:02.050419232Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000300553,
        "lastEvaluation": "2023-09-21T23:04:02.05041441Z"
      },
      {
        "name": "mcd-reboot-error",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "MCDRebootError",
            "query": "mcd_reboot_err > 0",
            "duration": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Reboot failed on {{ $labels.node }} , update may be blocked"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000585286,
            "lastEvaluation": "2023-09-21T23:04:08.321359608Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000602035,
        "lastEvaluation": "2023-09-21T23:04:08.321350566Z"
      },
      {
        "name": "system-memory-exceeds-reservation",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "SystemMemoryExceedsReservation",
            "query": "sum by(node) (container_memory_rss{id=\"/system.slice\"}) > ((sum by(node) (kube_node_status_capacity{resource=\"memory\"} - kube_node_status_allocatable{resource=\"memory\"})) * 0.9)",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "System memory usage of {{ $value | humanize }} on {{ $labels.node }} exceeds 90% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The reservation may be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html) when running nodes with high numbers of pods."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001132313,
            "lastEvaluation": "2023-09-21T23:03:56.927625126Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.001149869,
        "lastEvaluation": "2023-09-21T23:03:56.92761426Z"
      },
      {
        "name": "alertmanager.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "AlertmanagerConfigInconsistent",
            "query": "count by(namespace, service) (count_values by(namespace, service) (\"config_hash\", alertmanager_config_hash{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"})) != 1",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "The configuration of the instances of the Alertmanager cluster `{{ $labels.namespace }}/{{ $labels.service }}` are out of sync.\n{{ range printf \"alertmanager_config_hash{namespace=\\\"%s\\\",service=\\\"%s\\\"}\" $labels.namespace $labels.service | query }}\nConfiguration hash for pod {{ .Labels.pod }} is \"{{ printf \"%.f\" .Value }}\"\n{{ end }}\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000616163,
            "lastEvaluation": "2023-09-21T23:04:04.487040328Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerFailedReload",
            "query": "alertmanager_config_last_reload_successful{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"} == 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000174892,
            "lastEvaluation": "2023-09-21T23:04:04.487659459Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerMembersInconsistent",
            "query": "alertmanager_cluster_members{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"} != on(service) group_left() count by(service) (alertmanager_cluster_members{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"})",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Alertmanager has not found all other members of the cluster."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000251401,
            "lastEvaluation": "2023-09-21T23:04:04.487836219Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.001060958,
        "lastEvaluation": "2023-09-21T23:04:04.487030649Z"
      },
      {
        "name": "etcd",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "etcdMembersDown",
            "query": "max without(endpoint) (sum without(instance) (up{job=~\".*etcd.*\"} == bool 0) or count without(To) (sum without(instance) (rate(etcd_network_peer_sent_failures_total{job=~\".*etcd.*\"}[2m])) > 0.01)) > 0",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": members are down ({{ $value }}).",
              "summary": "etcd cluster members are down."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001640784,
            "lastEvaluation": "2023-09-21T23:03:45.35472636Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdNoLeader",
            "query": "etcd_server_has_leader{job=~\".*etcd.*\"} == 0",
            "duration": 60,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": member {{ $labels.instance }} has no leader.",
              "summary": "etcd cluster has no leader."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000554911,
            "lastEvaluation": "2023-09-21T23:03:45.356370871Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdHighNumberOfLeaderChanges",
            "query": "increase((max without(instance) (etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}) or 0 * absent(etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}))[15m:1m]) >= 4",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": {{ $value }} leader changes within the last 15 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.",
              "summary": "etcd cluster has high number of leader changes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001229014,
            "lastEvaluation": "2023-09-21T23:03:45.35692812Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdGRPCRequestsSlow",
            "query": "histogram_quantile(0.99, sum without(grpc_type) (rate(grpc_server_handling_seconds_bucket{grpc_type=\"unary\",job=~\".*etcd.*\"}[5m]))) > 0.15",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": gRPC requests to {{ $labels.grpc_method }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.",
              "summary": "etcd grpc requests are slow"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000577029,
            "lastEvaluation": "2023-09-21T23:03:45.358160834Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdMemberCommunicationSlow",
            "query": "histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.15",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.",
              "summary": "etcd cluster member communication is slow."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00352188,
            "lastEvaluation": "2023-09-21T23:03:45.358741016Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdHighNumberOfFailedProposals",
            "query": "rate(etcd_server_proposals_failed_total{job=~\".*etcd.*\"}[15m]) > 5",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}.",
              "summary": "etcd cluster has high number of proposal failures."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000422213,
            "lastEvaluation": "2023-09-21T23:03:45.362266797Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdHighFsyncDurations",
            "query": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.5",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.",
              "summary": "etcd cluster 99th percentile fsync durations are too high."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001684964,
            "lastEvaluation": "2023-09-21T23:03:45.362691241Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdHighFsyncDurations",
            "query": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 1",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001756079,
            "lastEvaluation": "2023-09-21T23:03:45.364379837Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdHighCommitDurations",
            "query": "histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.25",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "etcd cluster \"{{ $labels.job }}\": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.",
              "summary": "etcd cluster 99th percentile commit durations are too high."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000903778,
            "lastEvaluation": "2023-09-21T23:03:45.366137842Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdBackendQuotaLowSpace",
            "query": "(etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) * 100 > 95",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "etcd cluster \"{{ $labels.job }}\": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000240372,
            "lastEvaluation": "2023-09-21T23:03:45.367044493Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "etcdExcessiveDatabaseGrowth",
            "query": "increase(((etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) * 100)[4h:1m]) > 50",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "etcd cluster \"{{ $labels.job }}\": Observed surge in etcd writes leading to 50% increase in database size over the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001332137,
            "lastEvaluation": "2023-09-21T23:03:45.367285991Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.013905491,
        "lastEvaluation": "2023-09-21T23:03:45.354716022Z"
      },
      {
        "name": "general.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "firing",
            "name": "TargetDown",
            "query": "100 * (count by(job, namespace, service) (up == 0) / count by(job, namespace, service) (up)) > 10",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "TargetDown",
                  "job": "alertmanager-main",
                  "namespace": "openshift-monitoring",
                  "service": "alertmanager-main",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "100% of the alertmanager-main/alertmanager-main targets in openshift-monitoring namespace are down."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:00.163677339Z",
                "value": "1e+02"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.004580759,
            "lastEvaluation": "2023-09-21T23:04:00.1652944Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Watchdog",
            "query": "vector(1)",
            "duration": 0,
            "labels": {
              "severity": "none"
            },
            "annotations": {
              "message": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "Watchdog",
                  "severity": "none"
                },
                "annotations": {
                  "message": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:00.163677339Z",
                "value": "1e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000215678,
            "lastEvaluation": "2023-09-21T23:04:00.169879257Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.004811628,
        "lastEvaluation": "2023-09-21T23:04:00.165286245Z"
      },
      {
        "name": "k8s.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "namespace:container_cpu_usage_seconds_total:sum_rate",
            "query": "sum by(namespace) (rate(container_cpu_usage_seconds_total{container!=\"POD\",image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.009907803,
            "lastEvaluation": "2023-09-21T23:03:55.730216563Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate",
            "query": "sum by(cluster, namespace, pod, container) (rate(container_cpu_usage_seconds_total{container!=\"POD\",image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on(cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.010629792,
            "lastEvaluation": "2023-09-21T23:03:55.74012987Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod_container:container_memory_working_set_bytes",
            "query": "container_memory_working_set_bytes{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.013591283,
            "lastEvaluation": "2023-09-21T23:03:55.750762858Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod_container:container_memory_rss",
            "query": "container_memory_rss{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.01315332,
            "lastEvaluation": "2023-09-21T23:03:55.764357486Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod_container:container_memory_cache",
            "query": "container_memory_cache{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.012319596,
            "lastEvaluation": "2023-09-21T23:03:55.77751401Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod_container:container_memory_swap",
            "query": "container_memory_swap{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.012396006,
            "lastEvaluation": "2023-09-21T23:03:55.789836595Z",
            "type": "recording"
          },
          {
            "name": "namespace:container_memory_usage_bytes:sum",
            "query": "sum by(namespace) (container_memory_usage_bytes{container!=\"POD\",image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"})",
            "health": "ok",
            "evaluationTime": 0.003444525,
            "lastEvaluation": "2023-09-21T23:03:55.802235654Z",
            "type": "recording"
          },
          {
            "name": "namespace:kube_pod_container_resource_requests_memory_bytes:sum",
            "query": "sum by(namespace) (sum by(namespace, pod) (max by(namespace, pod, container) (kube_pod_container_resource_requests_memory_bytes{job=\"kube-state-metrics\"}) * on(namespace, pod) group_left() max by(namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
            "health": "ok",
            "evaluationTime": 0.005692631,
            "lastEvaluation": "2023-09-21T23:03:55.8056821Z",
            "type": "recording"
          },
          {
            "name": "namespace:kube_pod_container_resource_requests_cpu_cores:sum",
            "query": "sum by(namespace) (sum by(namespace, pod) (max by(namespace, pod, container) (kube_pod_container_resource_requests_cpu_cores{job=\"kube-state-metrics\"}) * on(namespace, pod) group_left() max by(namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
            "health": "ok",
            "evaluationTime": 0.00642896,
            "lastEvaluation": "2023-09-21T23:03:55.811376611Z",
            "type": "recording"
          },
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by(cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicaSet\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (1, max by(replicaset, namespace, owner_name) (kube_replicaset_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "deployment"
            },
            "health": "ok",
            "evaluationTime": 0.002941979,
            "lastEvaluation": "2023-09-21T23:03:55.817808064Z",
            "type": "recording"
          },
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"DaemonSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "daemonset"
            },
            "health": "ok",
            "evaluationTime": 0.0009547,
            "lastEvaluation": "2023-09-21T23:03:55.820753217Z",
            "type": "recording"
          },
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"StatefulSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "statefulset"
            },
            "health": "ok",
            "evaluationTime": 0.000250429,
            "lastEvaluation": "2023-09-21T23:03:55.821709885Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.091754644,
        "lastEvaluation": "2023-09-21T23:03:55.730207932Z"
      },
      {
        "name": "kube-apiserver-availability.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.05331708,
            "lastEvaluation": "2023-09-21T23:02:53.759229371Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.02447136,
            "lastEvaluation": "2023-09-21T23:02:53.812550692Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.003272175,
            "lastEvaluation": "2023-09-21T23:02:53.837025989Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.005624953,
            "lastEvaluation": "2023-09-21T23:02:53.840299821Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.001381684,
            "lastEvaluation": "2023-09-21T23:02:53.845927225Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.0008524,
            "lastEvaluation": "2023-09-21T23:02:53.84731077Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.00018593,
            "lastEvaluation": "2023-09-21T23:02:53.848164465Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000252161,
            "lastEvaluation": "2023-09-21T23:02:53.848351429Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000138337,
            "lastEvaluation": "2023-09-21T23:02:53.848604598Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000131405,
            "lastEvaluation": "2023-09-21T23:02:53.848743745Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000130545,
            "lastEvaluation": "2023-09-21T23:02:53.848875871Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000143341,
            "lastEvaluation": "2023-09-21T23:02:53.849007681Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000415675,
            "lastEvaluation": "2023-09-21T23:02:53.849151732Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.002097699,
            "lastEvaluation": "2023-09-21T23:02:53.849569056Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.001158507,
            "lastEvaluation": "2023-09-21T23:02:53.851668326Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.001279817,
            "lastEvaluation": "2023-09-21T23:02:53.852828567Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000206878,
            "lastEvaluation": "2023-09-21T23:02:53.854110034Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.002381852,
            "lastEvaluation": "2023-09-21T23:02:53.854317751Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000765274,
            "lastEvaluation": "2023-09-21T23:02:53.856701463Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.0021826,
            "lastEvaluation": "2023-09-21T23:02:53.857468439Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000683785,
            "lastEvaluation": "2023-09-21T23:02:53.85965288Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.001280053,
            "lastEvaluation": "2023-09-21T23:02:53.860338461Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.000705204,
            "lastEvaluation": "2023-09-21T23:02:53.861620961Z",
            "type": "recording"
          },
          {
            "name": "code_verb:apiserver_request_total:increase30d",
            "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
            "health": "ok",
            "evaluationTime": 0.001009109,
            "lastEvaluation": "2023-09-21T23:02:53.862327936Z",
            "type": "recording"
          },
          {
            "name": "code:apiserver_request_total:increase30d",
            "query": "sum by(code) (code_verb:apiserver_request_total:increase30d{verb=~\"LIST|GET\"})",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.000234409,
            "lastEvaluation": "2023-09-21T23:02:53.863339653Z",
            "type": "recording"
          },
          {
            "name": "code:apiserver_request_total:increase30d",
            "query": "sum by(code) (code_verb:apiserver_request_total:increase30d{verb=~\"POST|PUT|PATCH|DELETE\"})",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.000872348,
            "lastEvaluation": "2023-09-21T23:02:53.863575365Z",
            "type": "recording"
          }
        ],
        "interval": 180,
        "evaluationTime": 0.105229855,
        "lastEvaluation": "2023-09-21T23:02:53.759221929Z"
      },
      {
        "name": "kube-apiserver-slos",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeAPIErrorBudgetBurn",
            "query": "sum(apiserver_request:burnrate1h) > (14.4 * 0.01) and sum(apiserver_request:burnrate5m) > (14.4 * 0.01)",
            "duration": 120,
            "labels": {
              "long": "1h",
              "severity": "critical",
              "short": "5m"
            },
            "annotations": {
              "message": "The API server is burning too much error budget"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000992674,
            "lastEvaluation": "2023-09-21T23:03:48.517155196Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeAPIErrorBudgetBurn",
            "query": "sum(apiserver_request:burnrate6h) > (6 * 0.01) and sum(apiserver_request:burnrate30m) > (6 * 0.01)",
            "duration": 900,
            "labels": {
              "long": "6h",
              "severity": "critical",
              "short": "30m"
            },
            "annotations": {
              "message": "The API server is burning too much error budget"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000467036,
            "lastEvaluation": "2023-09-21T23:03:48.518150642Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeAPIErrorBudgetBurn",
            "query": "sum(apiserver_request:burnrate1d) > (3 * 0.01) and sum(apiserver_request:burnrate2h) > (3 * 0.01)",
            "duration": 3600,
            "labels": {
              "long": "1d",
              "severity": "warning",
              "short": "2h"
            },
            "annotations": {
              "message": "The API server is burning too much error budget"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000412141,
            "lastEvaluation": "2023-09-21T23:03:48.518621397Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeAPIErrorBudgetBurn",
            "query": "sum(apiserver_request:burnrate3d) > (1 * 0.01) and sum(apiserver_request:burnrate6h) > (1 * 0.01)",
            "duration": 10800,
            "labels": {
              "long": "3d",
              "severity": "warning",
              "short": "6h"
            },
            "annotations": {
              "message": "The API server is burning too much error budget"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000228508,
            "lastEvaluation": "2023-09-21T23:03:48.519035446Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.00213836,
        "lastEvaluation": "2023-09-21T23:03:48.517128615Z"
      },
      {
        "name": "kube-apiserver.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "apiserver_request:burnrate1d",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[1d])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[1d])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[1d])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[1d])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.162937908,
            "lastEvaluation": "2023-09-21T23:04:08.354175402Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate1h",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[1h])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[1h])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[1h])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[1h])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.025460269,
            "lastEvaluation": "2023-09-21T23:04:08.517118273Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate2h",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[2h])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[2h])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[2h])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[2h])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.0286056,
            "lastEvaluation": "2023-09-21T23:04:08.542581871Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate30m",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[30m])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[30m])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[30m])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[30m])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.02184773,
            "lastEvaluation": "2023-09-21T23:04:08.571192212Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate3d",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[3d])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[3d])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[3d])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[3d])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.142980603,
            "lastEvaluation": "2023-09-21T23:04:08.593043962Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate5m",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[5m])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[5m])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[5m])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[5m])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.016634532,
            "lastEvaluation": "2023-09-21T23:04:08.736030055Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate6h",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[6h])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[6h])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[6h])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[6h])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.024949216,
            "lastEvaluation": "2023-09-21T23:04:08.752667572Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate1d",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.032243014,
            "lastEvaluation": "2023-09-21T23:04:08.777619835Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate1h",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.007263064,
            "lastEvaluation": "2023-09-21T23:04:08.809866052Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate2h",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.005197371,
            "lastEvaluation": "2023-09-21T23:04:08.817131189Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate30m",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.004315492,
            "lastEvaluation": "2023-09-21T23:04:08.822330472Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate3d",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.031782645,
            "lastEvaluation": "2023-09-21T23:04:08.826647848Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate5m",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.003377263,
            "lastEvaluation": "2023-09-21T23:04:08.858432387Z",
            "type": "recording"
          },
          {
            "name": "apiserver_request:burnrate6h",
            "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.005109633,
            "lastEvaluation": "2023-09-21T23:04:08.861813717Z",
            "type": "recording"
          },
          {
            "name": "code_resource:apiserver_request_total:rate5m",
            "query": "sum by(code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))",
            "labels": {
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.006263677,
            "lastEvaluation": "2023-09-21T23:04:08.866925213Z",
            "type": "recording"
          },
          {
            "name": "code_resource:apiserver_request_total:rate5m",
            "query": "sum by(code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))",
            "labels": {
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.001598807,
            "lastEvaluation": "2023-09-21T23:04:08.873190792Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) > 0",
            "labels": {
              "quantile": "0.99",
              "verb": "read"
            },
            "health": "ok",
            "evaluationTime": 0.064201636,
            "lastEvaluation": "2023-09-21T23:04:08.874791352Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) > 0",
            "labels": {
              "quantile": "0.99",
              "verb": "write"
            },
            "health": "ok",
            "evaluationTime": 0.013209864,
            "lastEvaluation": "2023-09-21T23:04:08.93899774Z",
            "type": "recording"
          },
          {
            "name": "cluster:apiserver_request_duration_seconds:mean5m",
            "query": "sum without(instance, pod) (rate(apiserver_request_duration_seconds_sum{subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])) / sum without(instance, pod) (rate(apiserver_request_duration_seconds_count{subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.009407429,
            "lastEvaluation": "2023-09-21T23:04:08.952210233Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))",
            "labels": {
              "quantile": "0.99"
            },
            "health": "ok",
            "evaluationTime": 0.047017499,
            "lastEvaluation": "2023-09-21T23:04:08.961620458Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))",
            "labels": {
              "quantile": "0.9"
            },
            "health": "ok",
            "evaluationTime": 0.048894899,
            "lastEvaluation": "2023-09-21T23:04:09.008642414Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))",
            "labels": {
              "quantile": "0.5"
            },
            "health": "ok",
            "evaluationTime": 0.046325262,
            "lastEvaluation": "2023-09-21T23:04:09.057543308Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.749705043,
        "lastEvaluation": "2023-09-21T23:04:08.354169577Z"
      },
      {
        "name": "kube-prometheus-general.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "count:up1",
            "query": "count without(instance, pod, node) (up == 1)",
            "health": "ok",
            "evaluationTime": 0.00212121,
            "lastEvaluation": "2023-09-21T23:04:05.710607891Z",
            "type": "recording"
          },
          {
            "name": "count:up0",
            "query": "count without(instance, pod, node) (up == 0)",
            "health": "ok",
            "evaluationTime": 0.000779262,
            "lastEvaluation": "2023-09-21T23:04:05.712731928Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.002911462,
        "lastEvaluation": "2023-09-21T23:04:05.710602296Z"
      },
      {
        "name": "kube-prometheus-node-recording.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "instance:node_cpu:rate:sum",
            "query": "sum by(instance) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[3m]))",
            "health": "ok",
            "evaluationTime": 0.002486166,
            "lastEvaluation": "2023-09-21T23:03:47.885150237Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_receive_bytes:rate:sum",
            "query": "sum by(instance) (rate(node_network_receive_bytes_total[3m]))",
            "health": "ok",
            "evaluationTime": 0.002277765,
            "lastEvaluation": "2023-09-21T23:03:47.887642534Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_transmit_bytes:rate:sum",
            "query": "sum by(instance) (rate(node_network_transmit_bytes_total[3m]))",
            "health": "ok",
            "evaluationTime": 0.001933035,
            "lastEvaluation": "2023-09-21T23:03:47.889923559Z",
            "type": "recording"
          },
          {
            "name": "instance:node_cpu:ratio",
            "query": "sum without(cpu, mode) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[5m])) / on(instance) group_left() count by(instance) (sum by(instance, cpu) (node_cpu_seconds_total))",
            "health": "ok",
            "evaluationTime": 0.004448238,
            "lastEvaluation": "2023-09-21T23:03:47.891859473Z",
            "type": "recording"
          },
          {
            "name": "cluster:node_cpu:sum_rate5m",
            "query": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.001792728,
            "lastEvaluation": "2023-09-21T23:03:47.896330642Z",
            "type": "recording"
          },
          {
            "name": "cluster:node_cpu:ratio",
            "query": "cluster:node_cpu_seconds_total:rate5m / count(sum by(instance, cpu) (node_cpu_seconds_total))",
            "health": "ok",
            "evaluationTime": 0.001086657,
            "lastEvaluation": "2023-09-21T23:03:47.898125916Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.014076764,
        "lastEvaluation": "2023-09-21T23:03:47.885140213Z"
      },
      {
        "name": "kube-scheduler.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.99"
            },
            "health": "ok",
            "evaluationTime": 0.000434347,
            "lastEvaluation": "2023-09-21T23:03:52.317238075Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.99"
            },
            "health": "ok",
            "evaluationTime": 0.001661836,
            "lastEvaluation": "2023-09-21T23:03:52.317675366Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.99"
            },
            "health": "ok",
            "evaluationTime": 0.001686748,
            "lastEvaluation": "2023-09-21T23:03:52.319341144Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.9"
            },
            "health": "ok",
            "evaluationTime": 0.000243857,
            "lastEvaluation": "2023-09-21T23:03:52.321031672Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.9"
            },
            "health": "ok",
            "evaluationTime": 0.001649693,
            "lastEvaluation": "2023-09-21T23:03:52.32127761Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.9"
            },
            "health": "ok",
            "evaluationTime": 0.001133011,
            "lastEvaluation": "2023-09-21T23:03:52.322931835Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.5"
            },
            "health": "ok",
            "evaluationTime": 0.000114734,
            "lastEvaluation": "2023-09-21T23:03:52.324066534Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.5"
            },
            "health": "ok",
            "evaluationTime": 0.001355805,
            "lastEvaluation": "2023-09-21T23:03:52.32418228Z",
            "type": "recording"
          },
          {
            "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))",
            "labels": {
              "quantile": "0.5"
            },
            "health": "ok",
            "evaluationTime": 0.00087382,
            "lastEvaluation": "2023-09-21T23:03:52.325552598Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.009198957,
        "lastEvaluation": "2023-09-21T23:03:52.317230762Z"
      },
      {
        "name": "kube-state-metrics",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeStateMetricsListErrors",
            "query": "(sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001245749,
            "lastEvaluation": "2023-09-21T23:04:08.333972619Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStateMetricsWatchErrors",
            "query": "(sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001267974,
            "lastEvaluation": "2023-09-21T23:04:08.33522198Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.00252855,
        "lastEvaluation": "2023-09-21T23:04:08.333966958Z"
      },
      {
        "name": "kubelet.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
            "labels": {
              "quantile": "0.99"
            },
            "health": "ok",
            "evaluationTime": 0.002350203,
            "lastEvaluation": "2023-09-21T23:03:49.708376732Z",
            "type": "recording"
          },
          {
            "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.9, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
            "labels": {
              "quantile": "0.9"
            },
            "health": "ok",
            "evaluationTime": 0.002539123,
            "lastEvaluation": "2023-09-21T23:03:49.710731918Z",
            "type": "recording"
          },
          {
            "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.5, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
            "labels": {
              "quantile": "0.5"
            },
            "health": "ok",
            "evaluationTime": 0.001867405,
            "lastEvaluation": "2023-09-21T23:03:49.713275608Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.006781147,
        "lastEvaluation": "2023-09-21T23:03:49.708367234Z"
      },
      {
        "name": "kubernetes-apps",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "pending",
            "name": "KubePodCrashLooping",
            "query": "rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[5m]) * 60 * 5 > 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "KubePodCrashLooping",
                  "container": "alertmanager",
                  "endpoint": "https-main",
                  "job": "kube-state-metrics",
                  "namespace": "openshift-monitoring",
                  "pod": "alertmanager-main-1",
                  "service": "kube-state-metrics",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "Pod openshift-monitoring/alertmanager-main-1 (alertmanager) is restarting 1.25 times / 5 minutes."
                },
                "state": "pending",
                "activeAt": "2023-09-21T23:00:40.59085788Z",
                "value": "1.25e+00"
              },
              {
                "labels": {
                  "alertname": "KubePodCrashLooping",
                  "container": "alertmanager",
                  "endpoint": "https-main",
                  "job": "kube-state-metrics",
                  "namespace": "openshift-monitoring",
                  "pod": "alertmanager-main-0",
                  "service": "kube-state-metrics",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "Pod openshift-monitoring/alertmanager-main-0 (alertmanager) is restarting 1.25 times / 5 minutes."
                },
                "state": "pending",
                "activeAt": "2023-09-21T23:00:40.59085788Z",
                "value": "1.25e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.008211609,
            "lastEvaluation": "2023-09-21T23:03:40.594754475Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePodNotReady",
            "query": "sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",phase=~\"Pending|Unknown\"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!=\"Job\"}))) > 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.007879984,
            "lastEvaluation": "2023-09-21T23:03:40.602970967Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDeploymentGenerationMismatch",
            "query": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000927958,
            "lastEvaluation": "2023-09-21T23:03:40.610853236Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDeploymentReplicasMismatch",
            "query": "(kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) and (changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[5m]) == 0)",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001381722,
            "lastEvaluation": "2023-09-21T23:03:40.611782429Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "KubeStatefulSetReplicasMismatch",
            "query": "(kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[5m]) == 0)",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "KubeStatefulSetReplicasMismatch",
                  "container": "kube-rbac-proxy-main",
                  "endpoint": "https-main",
                  "job": "kube-state-metrics",
                  "namespace": "openshift-monitoring",
                  "service": "kube-state-metrics",
                  "severity": "warning",
                  "statefulset": "alertmanager-main"
                },
                "annotations": {
                  "message": "StatefulSet openshift-monitoring/alertmanager-main has not matched the expected number of replicas for longer than 15 minutes."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:40.59085788Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.00152438,
            "lastEvaluation": "2023-09-21T23:03:40.613165614Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStatefulSetGenerationMismatch",
            "query": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000330907,
            "lastEvaluation": "2023-09-21T23:03:40.614692484Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "KubeStatefulSetUpdateNotRolledOut",
            "query": "(max without(revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"})) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[5m]) == 0)",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "KubeStatefulSetUpdateNotRolledOut",
                  "container": "kube-rbac-proxy-main",
                  "endpoint": "https-main",
                  "job": "kube-state-metrics",
                  "namespace": "openshift-monitoring",
                  "service": "kube-state-metrics",
                  "severity": "warning",
                  "statefulset": "alertmanager-main"
                },
                "annotations": {
                  "message": "StatefulSet openshift-monitoring/alertmanager-main update has not been rolled out."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:40.59085788Z",
                "value": "3e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001206287,
            "lastEvaluation": "2023-09-21T23:03:40.615024206Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDaemonSetRolloutStuck",
            "query": "((kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) or (kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != 0) or (kube_daemonset_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) or (kube_daemonset_status_number_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"})) and (changes(kube_daemonset_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[5m]) == 0)",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002764498,
            "lastEvaluation": "2023-09-21T23:03:40.616232206Z",
            "type": "alerting"
          },
          {
            "state": "pending",
            "name": "KubeContainerWaiting",
            "query": "sum by(namespace, pod, container) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) > 0",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}} has been in waiting state for longer than 1 hour."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "KubeContainerWaiting",
                  "container": "alertmanager",
                  "namespace": "openshift-monitoring",
                  "pod": "alertmanager-main-2",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "Pod openshift-monitoring/alertmanager-main-2 container alertmanager has been in waiting state for longer than 1 hour."
                },
                "state": "pending",
                "activeAt": "2023-09-21T22:29:40.59085788Z",
                "value": "1e+00"
              },
              {
                "labels": {
                  "alertname": "KubeContainerWaiting",
                  "container": "alertmanager",
                  "namespace": "openshift-monitoring",
                  "pod": "alertmanager-main-0",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "Pod openshift-monitoring/alertmanager-main-0 container alertmanager has been in waiting state for longer than 1 hour."
                },
                "state": "pending",
                "activeAt": "2023-09-21T22:45:40.59085788Z",
                "value": "1e+00"
              },
              {
                "labels": {
                  "alertname": "KubeContainerWaiting",
                  "container": "alertmanager",
                  "namespace": "openshift-monitoring",
                  "pod": "alertmanager-main-1",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "Pod openshift-monitoring/alertmanager-main-1 container alertmanager has been in waiting state for longer than 1 hour."
                },
                "state": "pending",
                "activeAt": "2023-09-21T22:35:40.59085788Z",
                "value": "1e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.012172124,
            "lastEvaluation": "2023-09-21T23:03:40.618998175Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDaemonSetNotScheduled",
            "query": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000526957,
            "lastEvaluation": "2023-09-21T23:03:40.631172159Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDaemonSetMisScheduled",
            "query": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000229181,
            "lastEvaluation": "2023-09-21T23:03:40.631699902Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeJobCompletion",
            "query": "kube_job_spec_completions{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} - kube_job_status_succeeded{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
            "duration": 43200,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than 12 hours to complete."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000308242,
            "lastEvaluation": "2023-09-21T23:03:40.631929758Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeJobFailed",
            "query": "kube_job_failed{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000132293,
            "lastEvaluation": "2023-09-21T23:03:40.632238718Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeHpaReplicasMismatch",
            "query": "(kube_hpa_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_hpa_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) and changes(kube_hpa_status_current_replicas[15m]) == 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired number of replicas for longer than 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000349983,
            "lastEvaluation": "2023-09-21T23:03:40.632371539Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeHpaMaxedOut",
            "query": "kube_hpa_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} == kube_hpa_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max replicas for longer than 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00046644,
            "lastEvaluation": "2023-09-21T23:03:40.632723688Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.038500956,
        "lastEvaluation": "2023-09-21T23:03:40.594692397Z"
      },
      {
        "name": "kubernetes-resources",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeCPUOvercommit",
            "query": "sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum) / sum(kube_node_status_allocatable_cpu_cores) > (count(kube_node_status_allocatable_cpu_cores) - 1) / count(kube_node_status_allocatable_cpu_cores)",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001400784,
            "lastEvaluation": "2023-09-21T23:04:01.535867077Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeMemoryOvercommit",
            "query": "sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum) / sum(kube_node_status_allocatable_memory_bytes) > (count(kube_node_status_allocatable_memory_bytes) - 1) / count(kube_node_status_allocatable_memory_bytes)",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001054813,
            "lastEvaluation": "2023-09-21T23:04:01.537271792Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeCPUQuotaOvercommit",
            "query": "sum(kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",resource=\"cpu\",type=\"hard\"}) / sum(kube_node_status_allocatable_cpu_cores) > 1.5",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster has overcommitted CPU resource requests for Namespaces."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000590852,
            "lastEvaluation": "2023-09-21T23:04:01.53832882Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeMemoryQuotaOvercommit",
            "query": "sum(kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",resource=\"memory\",type=\"hard\"}) / sum(kube_node_status_allocatable_memory_bytes{job=\"node-exporter\"}) > 1.5",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster has overcommitted memory resource requests for Namespaces."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000397619,
            "lastEvaluation": "2023-09-21T23:04:01.538921701Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeQuotaAlmostFull",
            "query": "kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"hard\"} > 0) > 0.9 < 1",
            "duration": 900,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
              "summary": "Namespace quota is going to be full."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000665235,
            "lastEvaluation": "2023-09-21T23:04:01.539320768Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeQuotaFullyUsed",
            "query": "kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"hard\"} > 0) == 1",
            "duration": 900,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "message": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000681295,
            "lastEvaluation": "2023-09-21T23:04:01.53998756Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeQuotaExceeded",
            "query": "kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"hard\"} > 0) > 1",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
              "summary": "Namespace quota has exceeded the limits."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000576196,
            "lastEvaluation": "2023-09-21T23:04:01.540670656Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.005389676,
        "lastEvaluation": "2023-09-21T23:04:01.535860723Z"
      },
      {
        "name": "kubernetes-storage",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubePersistentVolumeFillingUp",
            "query": "kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} < 0.03",
            "duration": 60,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000931797,
            "lastEvaluation": "2023-09-21T23:03:50.078223279Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePersistentVolumeFillingUp",
            "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) < 0.15 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[6h], 4 * 24 * 3600) < 0",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001036867,
            "lastEvaluation": "2023-09-21T23:03:50.079157601Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePersistentVolumeErrors",
            "query": "kube_persistentvolume_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",phase=~\"Failed|Pending\"} > 0",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000413777,
            "lastEvaluation": "2023-09-21T23:03:50.080196363Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.002399767,
        "lastEvaluation": "2023-09-21T23:03:50.078213739Z"
      },
      {
        "name": "kubernetes-system",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeClientErrors",
            "query": "(sum by(instance, job) (rate(rest_client_requests_total{code=~\"5..\"}[5m])) / sum by(instance, job) (rate(rest_client_requests_total[5m]))) > 0.01",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.004156045,
            "lastEvaluation": "2023-09-21T23:04:02.039627869Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.004173303,
        "lastEvaluation": "2023-09-21T23:04:02.039616985Z"
      },
      {
        "name": "kubernetes-system-apiserver",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeClientCertificateExpiration",
            "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 5400",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "A client certificate used to authenticate to the apiserver is expiring in less than 1.5 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001612746,
            "lastEvaluation": "2023-09-21T23:03:50.908823349Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeClientCertificateExpiration",
            "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 3600",
            "duration": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "A client certificate used to authenticate to the apiserver is expiring in less than 1.0 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002085185,
            "lastEvaluation": "2023-09-21T23:03:50.910449711Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AggregatedAPIErrors",
            "query": "sum by(name, namespace) (increase(aggregator_unavailable_apiservice_count[5m])) > 2",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. The number of errors have increased for it in the past five minutes. High values indicate that the availability of the service changes too often."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000110796,
            "lastEvaluation": "2023-09-21T23:03:50.912536729Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AggregatedAPIDown",
            "query": "(1 - max by(name, namespace) (avg_over_time(aggregator_unavailable_apiservice[5m]))) * 100 < 90",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 5m."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002315814,
            "lastEvaluation": "2023-09-21T23:03:50.912648326Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeAPIDown",
            "query": "absent(up{job=\"apiserver\"} == 1)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "KubeAPI has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000149574,
            "lastEvaluation": "2023-09-21T23:03:50.914965815Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.006302967,
        "lastEvaluation": "2023-09-21T23:03:50.90881496Z"
      },
      {
        "name": "kubernetes-system-controller-manager",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeControllerManagerDown",
            "query": "absent(up{job=\"kube-controller-manager\"} == 1)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "KubeControllerManager has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000598556,
            "lastEvaluation": "2023-09-21T23:03:52.287547407Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000615229,
        "lastEvaluation": "2023-09-21T23:03:52.287536706Z"
      },
      {
        "name": "kubernetes-system-kubelet",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeNodeNotReady",
            "query": "kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "{{ $labels.node }} has been unready for more than 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000425639,
            "lastEvaluation": "2023-09-21T23:04:00.395749849Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeNodeUnreachable",
            "query": "(kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} unless ignoring(key, value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "{{ $labels.node }} is unreachable and some workloads may be rescheduled."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000700018,
            "lastEvaluation": "2023-09-21T23:04:00.396177572Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletTooManyPods",
            "query": "count by(node) ((kube_pod_status_phase{job=\"kube-state-metrics\",phase=\"Running\"} == 1) * on(instance, pod, namespace, cluster) group_left(node) topk by(instance, pod, namespace, cluster) (1, kube_pod_info{job=\"kube-state-metrics\"})) / max by(node) (kube_node_status_capacity_pods{job=\"kube-state-metrics\"} != 1) > 0.95",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.008450166,
            "lastEvaluation": "2023-09-21T23:04:00.396880903Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeNodeReadinessFlapping",
            "query": "sum by(node) (changes(kube_node_status_condition{condition=\"Ready\",status=\"true\"}[15m])) > 2",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000339544,
            "lastEvaluation": "2023-09-21T23:04:00.405334229Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletPlegDurationHigh",
            "query": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000169088,
            "lastEvaluation": "2023-09-21T23:04:00.405675583Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletPodStartUpLatencyHigh",
            "query": "histogram_quantile(0.99, sum by(instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m]))) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"} > 60",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001905379,
            "lastEvaluation": "2023-09-21T23:04:00.405845994Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletDown",
            "query": "absent(up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Kubelet has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000227196,
            "lastEvaluation": "2023-09-21T23:04:00.407753598Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.012243712,
        "lastEvaluation": "2023-09-21T23:04:00.395741059Z"
      },
      {
        "name": "kubernetes-system-scheduler",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeSchedulerDown",
            "query": "absent(up{job=\"scheduler\"} == 1)",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "KubeScheduler has disappeared from Prometheus target discovery."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000473654,
            "lastEvaluation": "2023-09-21T23:03:51.981943008Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.00048378,
        "lastEvaluation": "2023-09-21T23:03:51.981937503Z"
      },
      {
        "name": "kubernetes.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "pod:container_memory_usage_bytes:sum",
            "query": "sum by(pod, namespace) (container_memory_usage_bytes{container=\"\",pod!=\"\"})",
            "health": "ok",
            "evaluationTime": 0.003965793,
            "lastEvaluation": "2023-09-21T23:04:06.663600654Z",
            "type": "recording"
          },
          {
            "name": "pod:container_spec_cpu_shares:sum",
            "query": "sum by(pod, namespace) (container_spec_cpu_shares{container=\"\",pod!=\"\"})",
            "health": "ok",
            "evaluationTime": 0.00266992,
            "lastEvaluation": "2023-09-21T23:04:06.66756977Z",
            "type": "recording"
          },
          {
            "name": "pod:container_cpu_usage:sum",
            "query": "sum by(pod, namespace) (rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.003065789,
            "lastEvaluation": "2023-09-21T23:04:06.670241823Z",
            "type": "recording"
          },
          {
            "name": "pod:container_fs_usage_bytes:sum",
            "query": "sum by(pod, namespace) (container_fs_usage_bytes{pod!=\"\"})",
            "health": "ok",
            "evaluationTime": 0.004505244,
            "lastEvaluation": "2023-09-21T23:04:06.673309492Z",
            "type": "recording"
          },
          {
            "name": "namespace:container_memory_usage_bytes:sum",
            "query": "sum by(namespace) (container_memory_usage_bytes{container!=\"\"})",
            "health": "ok",
            "evaluationTime": 0.004054078,
            "lastEvaluation": "2023-09-21T23:04:06.677816723Z",
            "type": "recording"
          },
          {
            "name": "namespace:container_spec_cpu_shares:sum",
            "query": "sum by(namespace) (container_spec_cpu_shares{container!=\"\"})",
            "health": "ok",
            "evaluationTime": 0.003110461,
            "lastEvaluation": "2023-09-21T23:04:06.681872451Z",
            "type": "recording"
          },
          {
            "name": "namespace:container_cpu_usage:sum",
            "query": "sum by(namespace) (rate(container_cpu_usage_seconds_total{container!=\"\",container!=\"POD\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.003236976,
            "lastEvaluation": "2023-09-21T23:04:06.684984686Z",
            "type": "recording"
          },
          {
            "name": "cluster:memory_usage:ratio",
            "query": "sum by(cluster) (container_memory_usage_bytes{container=\"\",pod!=\"\"}) / sum by(cluster) (machine_memory_bytes)",
            "health": "ok",
            "evaluationTime": 0.002312278,
            "lastEvaluation": "2023-09-21T23:04:06.688223482Z",
            "type": "recording"
          },
          {
            "name": "cluster:container_spec_cpu_shares:ratio",
            "query": "sum(container_spec_cpu_shares{container=\"\",pod!=\"\"}) / 1000 / sum(machine_cpu_cores)",
            "health": "ok",
            "evaluationTime": 0.001764605,
            "lastEvaluation": "2023-09-21T23:04:06.690537013Z",
            "type": "recording"
          },
          {
            "name": "cluster:container_cpu_usage:ratio",
            "query": "sum(rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m])) / sum(machine_cpu_cores)",
            "health": "ok",
            "evaluationTime": 0.002424852,
            "lastEvaluation": "2023-09-21T23:04:06.692302685Z",
            "type": "recording"
          },
          {
            "name": "cluster:master_nodes",
            "query": "max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role=\"master\"})",
            "labels": {
              "label_node_role_kubernetes_io": "master",
              "label_node_role_kubernetes_io_master": "true"
            },
            "health": "ok",
            "evaluationTime": 0.00023763,
            "lastEvaluation": "2023-09-21T23:04:06.694728893Z",
            "type": "recording"
          },
          {
            "name": "cluster:infra_nodes",
            "query": "max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role=\"infra\"})",
            "labels": {
              "label_node_role_kubernetes_io_infra": "true"
            },
            "health": "ok",
            "evaluationTime": 0.000100966,
            "lastEvaluation": "2023-09-21T23:04:06.694967256Z",
            "type": "recording"
          },
          {
            "name": "cluster:master_infra_nodes",
            "query": "max without(endpoint, instance, job, pod, service) (cluster:master_nodes and on(node) cluster:infra_nodes)",
            "labels": {
              "label_node_role_kubernetes_io_infra": "true",
              "label_node_role_kubernetes_io_master": "true"
            },
            "health": "ok",
            "evaluationTime": 8.8615e-05,
            "lastEvaluation": "2023-09-21T23:04:06.695068828Z",
            "type": "recording"
          },
          {
            "name": "cluster:nodes_roles",
            "query": "cluster:master_infra_nodes or on(node) cluster:master_nodes or on(node) cluster:infra_nodes or on(node) max without(endpoint, instance, job, pod, service) (kube_node_labels)",
            "health": "ok",
            "evaluationTime": 0.000265675,
            "lastEvaluation": "2023-09-21T23:04:06.695157983Z",
            "type": "recording"
          },
          {
            "name": "cluster:hyperthread_enabled_nodes",
            "query": "kube_node_labels and on(node) (sum by(node, package, core) (label_replace(node_cpu_info, \"node\", \"$1\", \"instance\", \"(.*)\")) == 2)",
            "labels": {
              "label_node_hyperthread_enabled": "true"
            },
            "health": "ok",
            "evaluationTime": 0.000309348,
            "lastEvaluation": "2023-09-21T23:04:06.695424726Z",
            "type": "recording"
          },
          {
            "name": "cluster:virt_platform_nodes:sum",
            "query": "count by(type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name) (sum by(instance, type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name) (virt_platform))",
            "health": "ok",
            "evaluationTime": 0.000114311,
            "lastEvaluation": "2023-09-21T23:04:06.695734895Z",
            "type": "recording"
          },
          {
            "name": "cluster:capacity_cpu_cores:sum",
            "query": "sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) ((cluster:master_nodes * on(node) group_left() max by(node) (kube_node_status_capacity_cpu_cores)) or on(node) (max without(endpoint, instance, job, pod, service) (kube_node_labels) * on(node) group_left() max by(node) (kube_node_status_capacity_cpu_cores)))",
            "health": "ok",
            "evaluationTime": 0.000339073,
            "lastEvaluation": "2023-09-21T23:04:06.69584979Z",
            "type": "recording"
          },
          {
            "name": "cluster:cpu_core_hyperthreading",
            "query": "clamp_max(label_replace(sum by(instance, package, core) (node_cpu_info{core!=\"\",package!=\"\"} or label_replace(label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")) > 1, \"label_node_hyperthread_enabled\", \"true\", \"instance\", \"(.*)\") or on(instance, package) label_replace(sum by(instance, package, core) (label_replace(node_cpu_info{core!=\"\",package!=\"\"} or label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")) <= 1, \"label_node_hyperthread_enabled\", \"false\", \"instance\", \"(.*)\"), 1)",
            "health": "ok",
            "evaluationTime": 0.000680664,
            "lastEvaluation": "2023-09-21T23:04:06.696189663Z",
            "type": "recording"
          },
          {
            "name": "cluster:cpu_core_node_labels",
            "query": "topk by(node) (1, cluster:nodes_roles) * on(node) group_right(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) label_replace(cluster:cpu_core_hyperthreading, \"node\", \"$1\", \"instance\", \"(.*)\")",
            "health": "ok",
            "evaluationTime": 0.000373027,
            "lastEvaluation": "2023-09-21T23:04:06.696871827Z",
            "type": "recording"
          },
          {
            "name": "cluster:capacity_cpu_cores_hyperthread_enabled:sum",
            "query": "count by(label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled) (cluster:cpu_core_node_labels)",
            "health": "ok",
            "evaluationTime": 0.000114953,
            "lastEvaluation": "2023-09-21T23:04:06.697245945Z",
            "type": "recording"
          },
          {
            "name": "cluster:capacity_memory_bytes:sum",
            "query": "sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io) ((cluster:master_nodes * on(node) group_left() max by(node) (kube_node_status_capacity_memory_bytes)) or on(node) (max without(endpoint, instance, job, pod, service) (kube_node_labels) * on(node) group_left() max by(node) (kube_node_status_capacity_memory_bytes)))",
            "health": "ok",
            "evaluationTime": 0.000387503,
            "lastEvaluation": "2023-09-21T23:04:06.697361564Z",
            "type": "recording"
          },
          {
            "name": "cluster:cpu_usage_cores:sum",
            "query": "sum(1 - rate(node_cpu_seconds_total{mode=\"idle\"}[2m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~\"node-exporter.+\"})",
            "health": "ok",
            "evaluationTime": 0.000333674,
            "lastEvaluation": "2023-09-21T23:04:06.697750052Z",
            "type": "recording"
          },
          {
            "name": "cluster:memory_usage_bytes:sum",
            "query": "sum(node_memory_MemTotal_bytes{job=\"node-exporter\"} - node_memory_MemAvailable_bytes{job=\"node-exporter\"})",
            "health": "ok",
            "evaluationTime": 0.00051,
            "lastEvaluation": "2023-09-21T23:04:06.698085003Z",
            "type": "recording"
          },
          {
            "name": "workload:cpu_usage_cores:sum",
            "query": "sum(rate(container_cpu_usage_seconds_total{container=\"\",namespace!~\"openshift-.+\",pod!=\"\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.001181848,
            "lastEvaluation": "2023-09-21T23:04:06.698598047Z",
            "type": "recording"
          },
          {
            "name": "openshift:cpu_usage_cores:sum",
            "query": "cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum",
            "health": "ok",
            "evaluationTime": 0.000228906,
            "lastEvaluation": "2023-09-21T23:04:06.699781217Z",
            "type": "recording"
          },
          {
            "name": "workload:memory_usage_bytes:sum",
            "query": "sum(container_memory_working_set_bytes{container=\"\",namespace!~\"openshift-.+\",pod!=\"\"})",
            "health": "ok",
            "evaluationTime": 0.002669078,
            "lastEvaluation": "2023-09-21T23:04:06.700012596Z",
            "type": "recording"
          },
          {
            "name": "openshift:memory_usage_bytes:sum",
            "query": "cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum",
            "health": "ok",
            "evaluationTime": 0.000171304,
            "lastEvaluation": "2023-09-21T23:04:06.702683972Z",
            "type": "recording"
          },
          {
            "name": "cluster:node_instance_type_count:sum",
            "query": "sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (cluster:master_nodes or on(node) kube_node_labels)",
            "health": "ok",
            "evaluationTime": 0.000287178,
            "lastEvaluation": "2023-09-21T23:04:06.702856858Z",
            "type": "recording"
          },
          {
            "name": "cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum",
            "query": "sum by(provisioner) (topk by(namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_resource_requests_storage_bytes) * on(namespace, persistentvolumeclaim) group_right() topk by(namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))))",
            "health": "ok",
            "evaluationTime": 0.000286012,
            "lastEvaluation": "2023-09-21T23:04:06.703145975Z",
            "type": "recording"
          },
          {
            "name": "cluster:kubelet_volume_stats_used_bytes:provisioner:sum",
            "query": "sum by(provisioner) (topk by(namespace, persistentvolumeclaim) (1, kubelet_volume_stats_used_bytes) * on(namespace, persistentvolumeclaim) group_right() topk by(namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))))",
            "health": "ok",
            "evaluationTime": 0.000254951,
            "lastEvaluation": "2023-09-21T23:04:06.703432946Z",
            "type": "recording"
          },
          {
            "name": "instance:etcd_object_counts:sum",
            "query": "sum by(instance) (etcd_object_counts)",
            "health": "ok",
            "evaluationTime": 0.003553997,
            "lastEvaluation": "2023-09-21T23:04:06.703689379Z",
            "type": "recording"
          },
          {
            "name": "cluster:usage:resources:sum",
            "query": "topk(500, max by(resource) (etcd_object_counts))",
            "health": "ok",
            "evaluationTime": 0.003570925,
            "lastEvaluation": "2023-09-21T23:04:06.707245981Z",
            "type": "recording"
          },
          {
            "name": "cluster:usage:pods:terminal:workload:sum",
            "query": "count(count by(namespace, pod) (kube_pod_restart_policy{namespace!~\"openshift-.+\",type!=\"Always\"}))",
            "health": "ok",
            "evaluationTime": 0.000154245,
            "lastEvaluation": "2023-09-21T23:04:06.710819348Z",
            "type": "recording"
          },
          {
            "name": "cluster:usage:containers:sum",
            "query": "sum(max by(instance) (kubelet_containers_per_pod_count_sum))",
            "health": "ok",
            "evaluationTime": 0.000125006,
            "lastEvaluation": "2023-09-21T23:04:06.71097429Z",
            "type": "recording"
          },
          {
            "name": "node_role_os_version_machine:cpu_capacity_cores:sum",
            "query": "count by(label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (cluster:cpu_core_node_labels)",
            "health": "ok",
            "evaluationTime": 0.000187956,
            "lastEvaluation": "2023-09-21T23:04:06.711100637Z",
            "type": "recording"
          },
          {
            "name": "cluster:capacity_cpu_sockets_hyperthread_enabled:sum",
            "query": "count by(label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) (max by(node, package, label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) (cluster:cpu_core_node_labels))",
            "health": "ok",
            "evaluationTime": 0.000163298,
            "lastEvaluation": "2023-09-21T23:04:06.71128931Z",
            "type": "recording"
          },
          {
            "name": "node_role_os_version_machine:cpu_capacity_sockets:sum",
            "query": "count by(label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (max by(node, package, label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (cluster:cpu_core_node_labels))",
            "health": "ok",
            "evaluationTime": 0.000156844,
            "lastEvaluation": "2023-09-21T23:04:06.711453488Z",
            "type": "recording"
          },
          {
            "name": "cluster:alertmanager_routing_enabled:max",
            "query": "clamp_max(sum(alertmanager_integrations), 1)",
            "health": "ok",
            "evaluationTime": 4.5554e-05,
            "lastEvaluation": "2023-09-21T23:04:06.71161095Z",
            "type": "recording"
          },
          {
            "state": "firing",
            "name": "ClusterMonitoringOperatorReconciliationErrors",
            "query": "rate(cluster_monitoring_operator_reconcile_errors_total[15m]) * 100 / rate(cluster_monitoring_operator_reconcile_attempts_total[15m]) > 10",
            "duration": 1800,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Cluster Monitoring Operator is experiencing reconciliation error rate of {{ printf \"%0.0f\" $value }}%."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "ClusterMonitoringOperatorReconciliationErrors",
                  "container": "kube-rbac-proxy",
                  "endpoint": "https",
                  "instance": "10.8.0.19:8443",
                  "job": "cluster-monitoring-operator",
                  "namespace": "openshift-monitoring",
                  "pod": "cluster-monitoring-operator-575dfd576f-szccx",
                  "service": "cluster-monitoring-operator",
                  "severity": "warning"
                },
                "annotations": {
                  "message": "Cluster Monitoring Operator is experiencing reconciliation error rate of 100%."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:14:06.662770393Z",
                "value": "1e+02"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000274835,
            "lastEvaluation": "2023-09-21T23:04:06.711657393Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerReceiversNotConfigured",
            "query": "cluster:alertmanager_routing_enabled:max == 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 5.2873e-05,
            "lastEvaluation": "2023-09-21T23:04:06.711933224Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "MultipleContainersOOMKilled",
            "query": "sum(max by(namespace, container, pod) (increase(kube_pod_container_status_restarts_total[12m])) and max by(namespace, container, pod) (kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}) == 1) > 5",
            "duration": 900,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "message": "Multiple containers were out of memory killed within the past 15 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.004624768,
            "lastEvaluation": "2023-09-21T23:04:06.711986701Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.053028512,
        "lastEvaluation": "2023-09-21T23:04:06.663586235Z"
      },
      {
        "name": "node-exporter",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "NodeFilesystemSpaceFillingUp",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.",
              "summary": "Filesystem is predicted to run out of space within the next 24 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002790276,
            "lastEvaluation": "2023-09-21T23:04:06.639288282Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemSpaceFillingUp",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 15 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.",
              "summary": "Filesystem is predicted to run out of space within the next 4 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002399931,
            "lastEvaluation": "2023-09-21T23:04:06.642081542Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfSpace",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
              "summary": "Filesystem has less than 5% space left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.003255031,
            "lastEvaluation": "2023-09-21T23:04:06.644484039Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfSpace",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
              "summary": "Filesystem has less than 3% space left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001619682,
            "lastEvaluation": "2023-09-21T23:04:06.647743351Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemFilesFillingUp",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.",
              "summary": "Filesystem is predicted to run out of inodes within the next 24 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002161384,
            "lastEvaluation": "2023-09-21T23:04:06.649365109Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemFilesFillingUp",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.",
              "summary": "Filesystem is predicted to run out of inodes within the next 4 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001970924,
            "lastEvaluation": "2023-09-21T23:04:06.651528778Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfFiles",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
              "summary": "Filesystem has less than 5% inodes left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.004212796,
            "lastEvaluation": "2023-09-21T23:04:06.653501495Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfFiles",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
            "duration": 3600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
              "summary": "Filesystem has less than 3% inodes left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001257501,
            "lastEvaluation": "2023-09-21T23:04:06.657716245Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeNetworkReceiveErrs",
            "query": "increase(node_network_receive_errs_total[2m]) > 10",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.",
              "summary": "Network interface is reporting many receive errors."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000719354,
            "lastEvaluation": "2023-09-21T23:04:06.658975335Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeNetworkTransmitErrs",
            "query": "increase(node_network_transmit_errs_total[2m]) > 10",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.",
              "summary": "Network interface is reporting many transmit errors."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000635825,
            "lastEvaluation": "2023-09-21T23:04:06.659695955Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeHighNumberConntrackEntriesUsed",
            "query": "(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $value | humanizePercentage }} of conntrack entries are used.",
              "summary": "Number of conntrack are getting close to the limit."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000210631,
            "lastEvaluation": "2023-09-21T23:04:06.660332638Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeTextFileCollectorScrapeError",
            "query": "node_textfile_scrape_error{job=\"node-exporter\"} == 1",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Node Exporter text file collector failed to scrape.",
              "summary": "Node Exporter text file collector failed to scrape."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 8.4943e-05,
            "lastEvaluation": "2023-09-21T23:04:06.660544511Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeClockSkewDetected",
            "query": "(node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.",
              "summary": "Clock skew detected."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000226804,
            "lastEvaluation": "2023-09-21T23:04:06.660630018Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeClockNotSynchronising",
            "query": "min_over_time(node_timex_sync_status[5m]) == 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.",
              "summary": "Clock not synchronising."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 7.6068e-05,
            "lastEvaluation": "2023-09-21T23:04:06.660857397Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeRAIDDegraded",
            "query": "node_md_disks_required - ignoring(state) (node_md_disks{state=\"active\"}) > 0",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.",
              "summary": "RAID Array is degraded"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000146408,
            "lastEvaluation": "2023-09-21T23:04:06.660934283Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeRAIDDiskFailure",
            "query": "node_md_disks{state=\"fail\"} > 0",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "At least one device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.",
              "summary": "Failed device in RAID array"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.0001063,
            "lastEvaluation": "2023-09-21T23:04:06.661082031Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.021909581,
        "lastEvaluation": "2023-09-21T23:04:06.639280918Z"
      },
      {
        "name": "node-exporter.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "instance:node_num_cpu:sum",
            "query": "count without(cpu) (count without(mode) (node_cpu_seconds_total{job=\"node-exporter\"}))",
            "health": "ok",
            "evaluationTime": 0.000974557,
            "lastEvaluation": "2023-09-21T23:03:52.523529317Z",
            "type": "recording"
          },
          {
            "name": "instance:node_cpu_utilisation:rate1m",
            "query": "1 - avg without(cpu, mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"}[1m]))",
            "health": "ok",
            "evaluationTime": 0.000307636,
            "lastEvaluation": "2023-09-21T23:03:52.52450578Z",
            "type": "recording"
          },
          {
            "name": "instance:node_load1_per_cpu:ratio",
            "query": "(node_load1{job=\"node-exporter\"} / instance:node_num_cpu:sum{job=\"node-exporter\"})",
            "health": "ok",
            "evaluationTime": 0.000266481,
            "lastEvaluation": "2023-09-21T23:03:52.524814513Z",
            "type": "recording"
          },
          {
            "name": "instance:node_memory_utilisation:ratio",
            "query": "1 - (node_memory_MemAvailable_bytes{job=\"node-exporter\"} / node_memory_MemTotal_bytes{job=\"node-exporter\"})",
            "health": "ok",
            "evaluationTime": 0.000257092,
            "lastEvaluation": "2023-09-21T23:03:52.525082576Z",
            "type": "recording"
          },
          {
            "name": "instance:node_vmstat_pgmajfault:rate1m",
            "query": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[1m])",
            "health": "ok",
            "evaluationTime": 0.000165274,
            "lastEvaluation": "2023-09-21T23:03:52.525340621Z",
            "type": "recording"
          },
          {
            "name": "instance_device:node_disk_io_time_seconds:rate1m",
            "query": "rate(node_disk_io_time_seconds_total{device=~\"nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])",
            "health": "ok",
            "evaluationTime": 0.000285369,
            "lastEvaluation": "2023-09-21T23:03:52.525506806Z",
            "type": "recording"
          },
          {
            "name": "instance_device:node_disk_io_time_weighted_seconds:rate1m",
            "query": "rate(node_disk_io_time_weighted_seconds_total{device=~\"nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])",
            "health": "ok",
            "evaluationTime": 0.000252889,
            "lastEvaluation": "2023-09-21T23:03:52.525793083Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_receive_bytes_excluding_lo:rate1m",
            "query": "sum without(device) (rate(node_network_receive_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
            "health": "ok",
            "evaluationTime": 0.000879387,
            "lastEvaluation": "2023-09-21T23:03:52.526046874Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_transmit_bytes_excluding_lo:rate1m",
            "query": "sum without(device) (rate(node_network_transmit_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
            "health": "ok",
            "evaluationTime": 0.000848756,
            "lastEvaluation": "2023-09-21T23:03:52.526927528Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_receive_drop_excluding_lo:rate1m",
            "query": "sum without(device) (rate(node_network_receive_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
            "health": "ok",
            "evaluationTime": 0.000715514,
            "lastEvaluation": "2023-09-21T23:03:52.52777745Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_transmit_drop_excluding_lo:rate1m",
            "query": "sum without(device) (rate(node_network_transmit_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
            "health": "ok",
            "evaluationTime": 0.000706799,
            "lastEvaluation": "2023-09-21T23:03:52.528494176Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.005678516,
        "lastEvaluation": "2023-09-21T23:03:52.523525243Z"
      },
      {
        "name": "node-network",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "NodeNetworkInterfaceFlapping",
            "query": "changes(node_network_up{device!~\"veth.+\",job=\"node-exporter\"}[2m]) > 2",
            "duration": 120,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Network interface \"{{ $labels.device }}\" changing it's up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}\""
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000815581,
            "lastEvaluation": "2023-09-21T23:03:59.409688345Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000834301,
        "lastEvaluation": "2023-09-21T23:03:59.409673479Z"
      },
      {
        "name": "node.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": ":kube_pod_info_node_count:",
            "query": "sum(min by(cluster, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.00376046,
            "lastEvaluation": "2023-09-21T23:04:05.294990302Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod:kube_pod_info:",
            "query": "topk by(namespace, pod) (1, max by(node, namespace, pod) (label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")))",
            "health": "ok",
            "evaluationTime": 0.007552105,
            "lastEvaluation": "2023-09-21T23:04:05.29875633Z",
            "type": "recording"
          },
          {
            "name": "node:node_num_cpu:sum",
            "query": "count by(cluster, node) (sum by(node, cpu) (node_cpu_seconds_total{job=\"node-exporter\"} * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:))",
            "health": "ok",
            "evaluationTime": 0.006040032,
            "lastEvaluation": "2023-09-21T23:04:05.306313811Z",
            "type": "recording"
          },
          {
            "name": ":node_memory_MemAvailable_bytes:sum",
            "query": "sum by(cluster) (node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))",
            "health": "ok",
            "evaluationTime": 0.00059723,
            "lastEvaluation": "2023-09-21T23:04:05.312357426Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.017977729,
        "lastEvaluation": "2023-09-21T23:04:05.294980879Z"
      },
      {
        "name": "openshift-build.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "build_error_rate",
            "query": "sum(openshift_build_total{job=\"kubernetes-apiservers\",phase=\"Error\"}) / (sum(openshift_build_total{job=\"kubernetes-apiservers\",phase=~\"Failed|Complete|Error\"}))",
            "health": "ok",
            "evaluationTime": 0.000567838,
            "lastEvaluation": "2023-09-21T23:03:59.299567998Z",
            "type": "recording"
          },
          {
            "name": "openshift:build_by_strategy:sum",
            "query": "sum by(strategy) (openshift_build_status_phase_total)",
            "health": "ok",
            "evaluationTime": 0.000134845,
            "lastEvaluation": "2023-09-21T23:03:59.300138898Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000720678,
        "lastEvaluation": "2023-09-21T23:03:59.299557476Z"
      },
      {
        "name": "openshift-etcd.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "etcdInsufficientMembers",
            "query": "sum without(instance, pod) (up{job=\"etcd\"} == bool 1 and etcd_server_has_leader{job=\"etcd\"} == bool 1) < ((count without(instance, pod) (up{job=\"etcd\"}) + 1) / 2)",
            "duration": 180,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "etcd is reporting fewer instances are available than are needed ({{ $value }}). When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed. This can occur when multiple control plane nodes are powered off or are unable to connect to each other via the network. Check that all control plane nodes are powered on and that network connections between each machine are functional.",
              "summary": "etcd is reporting that a majority of instances are unavailable."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001129035,
            "lastEvaluation": "2023-09-21T23:03:47.170335478Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.001147125,
        "lastEvaluation": "2023-09-21T23:03:47.17032347Z"
      },
      {
        "name": "openshift-ingress.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "code:cluster:ingress_http_request_count:rate5m:sum",
            "query": "sum by(code) (rate(haproxy_server_http_responses_total[5m]) > 0)",
            "health": "ok",
            "evaluationTime": 0.001370793,
            "lastEvaluation": "2023-09-21T23:03:55.898283115Z",
            "type": "recording"
          },
          {
            "name": "frontend:cluster:ingress_frontend_bytes_in:rate5m:sum",
            "query": "sum by(frontend) (rate(haproxy_frontend_bytes_in_total[5m]))",
            "health": "ok",
            "evaluationTime": 0.000255649,
            "lastEvaluation": "2023-09-21T23:03:55.899656456Z",
            "type": "recording"
          },
          {
            "name": "frontend:cluster:ingress_frontend_bytes_out:rate5m:sum",
            "query": "sum by(frontend) (rate(haproxy_frontend_bytes_out_total[5m]))",
            "health": "ok",
            "evaluationTime": 0.000205197,
            "lastEvaluation": "2023-09-21T23:03:55.899913007Z",
            "type": "recording"
          },
          {
            "name": "frontend:cluster:ingress_frontend_connections:sum",
            "query": "sum by(frontend) (haproxy_frontend_current_sessions)",
            "health": "ok",
            "evaluationTime": 0.000211894,
            "lastEvaluation": "2023-09-21T23:03:55.900119024Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.00205824,
        "lastEvaluation": "2023-09-21T23:03:55.898276177Z"
      },
      {
        "name": "openshift-monitoring.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "openshift:prometheus_tsdb_head_series:sum",
            "query": "sum by(job, namespace) (prometheus_tsdb_head_series{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"})",
            "health": "ok",
            "evaluationTime": 0.000664762,
            "lastEvaluation": "2023-09-21T23:03:57.58568119Z",
            "type": "recording"
          },
          {
            "name": "openshift:prometheus_tsdb_head_samples_appended_total:sum",
            "query": "sum by(job, namespace) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[2m]))",
            "health": "ok",
            "evaluationTime": 0.000605761,
            "lastEvaluation": "2023-09-21T23:03:57.586350293Z",
            "type": "recording"
          },
          {
            "name": "monitoring:container_memory_working_set_bytes:sum",
            "query": "sum by(namespace) (container_memory_working_set_bytes{container=\"\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"})",
            "health": "ok",
            "evaluationTime": 0.002228063,
            "lastEvaluation": "2023-09-21T23:03:57.586958375Z",
            "type": "recording"
          },
          {
            "name": "monitoring:haproxy_server_http_responses_total:sum",
            "query": "sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace=\"openshift-monitoring\",exported_service=~\"alertmanager-main|grafana|prometheus-k8s\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.00150971,
            "lastEvaluation": "2023-09-21T23:03:57.589189577Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.005032168,
        "lastEvaluation": "2023-09-21T23:03:57.585673148Z"
      },
      {
        "name": "openshift-sre.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "code:apiserver_request_total:rate:sum",
            "query": "sum by(code) (rate(apiserver_request_total{job=\"apiserver\"}[10m]))",
            "health": "ok",
            "evaluationTime": 0.018002527,
            "lastEvaluation": "2023-09-21T23:03:43.636827425Z",
            "type": "recording"
          },
          {
            "name": "code:registry_api_request_count:rate:sum",
            "query": "sum by(code) (rate(apiserver_request_total{job=\"apiserver\",resource=~\"image.*\",verb!=\"WATCH\"}[10m]))",
            "health": "ok",
            "evaluationTime": 0.000803182,
            "lastEvaluation": "2023-09-21T23:03:43.654832767Z",
            "type": "recording"
          },
          {
            "name": "kube_pod_status_ready:etcd:sum",
            "query": "sum by(condition) (kube_pod_status_ready{condition=\"true\",namespace=\"openshift-etcd\",pod=~\"etcd.*\"})",
            "health": "ok",
            "evaluationTime": 0.000213232,
            "lastEvaluation": "2023-09-21T23:03:43.655637684Z",
            "type": "recording"
          },
          {
            "name": "kube_pod_status_ready:image_registry:sum",
            "query": "sum by(condition) (kube_pod_status_ready{condition=\"true\",namespace=\"openshift-image-registry\",pod=~\"image-registry.*\"})",
            "health": "ok",
            "evaluationTime": 0.000131722,
            "lastEvaluation": "2023-09-21T23:03:43.655851995Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.019165894,
        "lastEvaluation": "2023-09-21T23:03:43.63682013Z"
      },
      {
        "name": "prometheus",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "PrometheusBadConfig",
            "query": "max_over_time(prometheus_config_last_reload_successful{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) == 0",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.",
              "summary": "Failed Prometheus configuration reload."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000658299,
            "lastEvaluation": "2023-09-21T23:03:56.736598329Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusNotificationQueueRunningFull",
            "query": "(predict_linear(prometheus_notifications_queue_length{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.",
              "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000583596,
            "lastEvaluation": "2023-09-21T23:03:56.737258403Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
            "query": "(rate(prometheus_notifications_errors_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / rate(prometheus_notifications_sent_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) * 100 > 1",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.",
              "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
            },
            "alerts": [
              {
                "labels": {
                  "alertmanager": "https://10.9.0.14:9095/api/v2/alerts",
                  "alertname": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.10.0.8:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-0",
                  "service": "prometheus-k8s",
                  "severity": "warning"
                },
                "annotations": {
                  "description": "73.3% errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-0 to Alertmanager https://10.9.0.14:9095/api/v2/alerts.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:09:56.728613546Z",
                "value": "7.333333333333334e+01"
              },
              {
                "labels": {
                  "alertmanager": "https://10.10.0.4:9095/api/v2/alerts",
                  "alertname": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.8.0.29:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-1",
                  "service": "prometheus-k8s",
                  "severity": "warning"
                },
                "annotations": {
                  "description": "73.3% errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-1 to Alertmanager https://10.10.0.4:9095/api/v2/alerts.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:56.728613546Z",
                "value": "7.333333333333334e+01"
              },
              {
                "labels": {
                  "alertmanager": "https://10.8.0.35:9095/api/v2/alerts",
                  "alertname": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.8.0.29:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-1",
                  "service": "prometheus-k8s",
                  "severity": "warning"
                },
                "annotations": {
                  "description": "73.3% errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-1 to Alertmanager https://10.8.0.35:9095/api/v2/alerts.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:56.728613546Z",
                "value": "7.333333333333334e+01"
              },
              {
                "labels": {
                  "alertmanager": "https://10.9.0.14:9095/api/v2/alerts",
                  "alertname": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.8.0.29:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-1",
                  "service": "prometheus-k8s",
                  "severity": "warning"
                },
                "annotations": {
                  "description": "73.3% errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-1 to Alertmanager https://10.9.0.14:9095/api/v2/alerts.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:56.728613546Z",
                "value": "7.333333333333334e+01"
              },
              {
                "labels": {
                  "alertmanager": "https://10.10.0.4:9095/api/v2/alerts",
                  "alertname": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.10.0.8:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-0",
                  "service": "prometheus-k8s",
                  "severity": "warning"
                },
                "annotations": {
                  "description": "73.3% errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-0 to Alertmanager https://10.10.0.4:9095/api/v2/alerts.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:09:56.728613546Z",
                "value": "7.333333333333334e+01"
              },
              {
                "labels": {
                  "alertmanager": "https://10.8.0.35:9095/api/v2/alerts",
                  "alertname": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.10.0.8:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-0",
                  "service": "prometheus-k8s",
                  "severity": "warning"
                },
                "annotations": {
                  "description": "73.3% errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-0 to Alertmanager https://10.8.0.35:9095/api/v2/alerts.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:09:56.728613546Z",
                "value": "7.333333333333334e+01"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.002353612,
            "lastEvaluation": "2023-09-21T23:03:56.737844923Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "PrometheusErrorSendingAlertsToAnyAlertmanager",
            "query": "min without(alertmanager) (rate(prometheus_notifications_errors_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / rate(prometheus_notifications_sent_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) * 100 > 3",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "{{ printf \"%.1f\" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.",
              "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "PrometheusErrorSendingAlertsToAnyAlertmanager",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.8.0.29:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-1",
                  "service": "prometheus-k8s",
                  "severity": "critical"
                },
                "annotations": {
                  "description": "73.3% minimum errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-1 to any Alertmanager.",
                  "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:08:56.728613546Z",
                "value": "7.333333333333334e+01"
              },
              {
                "labels": {
                  "alertname": "PrometheusErrorSendingAlertsToAnyAlertmanager",
                  "container": "prometheus-proxy",
                  "endpoint": "web",
                  "instance": "10.10.0.8:9091",
                  "job": "prometheus-k8s",
                  "namespace": "openshift-monitoring",
                  "pod": "prometheus-k8s-0",
                  "service": "prometheus-k8s",
                  "severity": "critical"
                },
                "annotations": {
                  "description": "73.3% minimum errors while sending alerts from Prometheus openshift-monitoring/prometheus-k8s-0 to any Alertmanager.",
                  "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
                },
                "state": "firing",
                "activeAt": "2023-09-21T22:09:56.728613546Z",
                "value": "7.333333333333334e+01"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000765095,
            "lastEvaluation": "2023-09-21T23:03:56.740201516Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusNotConnectedToAlertmanagers",
            "query": "max_over_time(prometheus_notifications_alertmanagers_discovered{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) < 1",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.",
              "summary": "Prometheus is not connected to any Alertmanagers."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000139487,
            "lastEvaluation": "2023-09-21T23:03:56.74096888Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusTSDBReloadsFailing",
            "query": "increase(prometheus_tsdb_reloads_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) > 0",
            "duration": 14400,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.",
              "summary": "Prometheus has issues reloading blocks from disk."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000128691,
            "lastEvaluation": "2023-09-21T23:03:56.741109205Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusTSDBCompactionsFailing",
            "query": "increase(prometheus_tsdb_compactions_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) > 0",
            "duration": 14400,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.",
              "summary": "Prometheus has issues compacting blocks."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000128931,
            "lastEvaluation": "2023-09-21T23:03:56.741238556Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusNotIngestingSamples",
            "query": "rate(prometheus_tsdb_head_samples_appended_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) <= 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.",
              "summary": "Prometheus is not ingesting samples."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000126859,
            "lastEvaluation": "2023-09-21T23:03:56.741368056Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusDuplicateTimestamps",
            "query": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
              "summary": "Prometheus is dropping samples with duplicate timestamps."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 9.4657e-05,
            "lastEvaluation": "2023-09-21T23:03:56.741495491Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOutOfOrderTimestamps",
            "query": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0",
            "duration": 3600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
              "summary": "Prometheus drops samples with out-of-order timestamps."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 8.4425e-05,
            "lastEvaluation": "2023-09-21T23:03:56.74159112Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRemoteStorageFailures",
            "query": "(rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / (rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) + rate(prometheus_remote_storage_succeeded_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))) * 100 > 1",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}",
              "summary": "Prometheus fails to send samples to remote storage."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000487309,
            "lastEvaluation": "2023-09-21T23:03:56.741676103Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRemoteWriteBehind",
            "query": "(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) - on(job, instance) group_right() max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) > 120",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.",
              "summary": "Prometheus remote write is behind."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000782908,
            "lastEvaluation": "2023-09-21T23:03:56.742166804Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRemoteWriteDesiredShards",
            "query": "(max_over_time(prometheus_remote_storage_shards_desired{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=~\"prometheus-k8s|prometheus-user-workload\"}` $labels.instance | query | first | value }}.",
              "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000411785,
            "lastEvaluation": "2023-09-21T23:03:56.742953308Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRuleFailures",
            "query": "increase(prometheus_rule_evaluation_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0",
            "duration": 900,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
              "summary": "Prometheus is failing rule evaluations."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000974348,
            "lastEvaluation": "2023-09-21T23:03:56.743366521Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusMissingRuleEvaluations",
            "query": "increase(prometheus_rule_group_iterations_missed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
              "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000964328,
            "lastEvaluation": "2023-09-21T23:03:56.744342292Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.008720983,
        "lastEvaluation": "2023-09-21T23:03:56.736588796Z"
      },
      {
        "name": "prometheus-operator",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "PrometheusOperatorListErrors",
            "query": "(sum by(controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_list_operations_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[10m]))) > 0.4",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001713304,
            "lastEvaluation": "2023-09-21T23:03:40.855471932Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorWatchErrors",
            "query": "(sum by(controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_watch_operations_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[10m]))) > 0.4",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Errors while performing Watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001086939,
            "lastEvaluation": "2023-09-21T23:03:40.857189352Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorReconcileErrors",
            "query": "rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[5m]) > 0.1",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace }} Namespace."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000438725,
            "lastEvaluation": "2023-09-21T23:03:40.858280232Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorNodeLookupErrors",
            "query": "rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[5m]) > 0.1",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000233009,
            "lastEvaluation": "2023-09-21T23:03:40.858721623Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.003498298,
        "lastEvaluation": "2023-09-21T23:03:40.855461001Z"
      },
      {
        "name": "telemeter.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-telemetry.yaml",
        "rules": [
          {
            "name": "cluster:telemetry_selected_series:count",
            "query": "count({__name__=~\"cluster:usage:.*|count:up0|count:up1|cluster_version|cluster_version_available_updates|cluster_operator_up|cluster_operator_conditions|cluster_version_payload|cluster_installer|cluster_infrastructure_provider|cluster_feature_set|instance:etcd_object_counts:sum|ALERTS|code:apiserver_request_total:rate:sum|cluster:capacity_cpu_cores:sum|cluster:capacity_memory_bytes:sum|cluster:cpu_usage_cores:sum|cluster:memory_usage_bytes:sum|openshift:cpu_usage_cores:sum|openshift:memory_usage_bytes:sum|workload:cpu_usage_cores:sum|workload:memory_usage_bytes:sum|cluster:virt_platform_nodes:sum|cluster:node_instance_type_count:sum|cnv:vmi_status_running:count|node_role_os_version_machine:cpu_capacity_cores:sum|node_role_os_version_machine:cpu_capacity_sockets:sum|subscription_sync_total|olm_resolution_duration_seconds|csv_succeeded|csv_abnormal|cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum|cluster:kubelet_volume_stats_used_bytes:provisioner:sum|ceph_cluster_total_bytes|ceph_cluster_total_used_raw_bytes|ceph_health_status|job:ceph_osd_metadata:count|job:kube_pv:count|job:ceph_pools_iops:total|job:ceph_pools_iops_bytes:total|job:ceph_versions_running:count|job:noobaa_total_unhealthy_buckets:sum|job:noobaa_bucket_count:sum|job:noobaa_total_object_count:sum|noobaa_accounts_num|noobaa_total_usage|console_url|cluster:network_attachment_definition_instances:max|cluster:network_attachment_definition_enabled_instance_up:max|insightsclient_request_send_total|cam_app_workload_migrations|cluster:apiserver_current_inflight_requests:sum:max_over_time:2m|cluster:telemetry_selected_series:count|openshift:prometheus_tsdb_head_series:sum|openshift:prometheus_tsdb_head_samples_appended_total:sum|monitoring:container_memory_working_set_bytes:sum|monitoring:haproxy_server_http_responses_total:sum|rhmi_status|cluster_legacy_scheduler_policy|cluster_master_schedulable|che_workspace_status|che_workspace_started_total|che_workspace_failure_total|che_workspace_start_time_seconds_sum|che_workspace_start_time_seconds_count|:apiserver_v1_image_imports:sum\",alertstate=~\"firing|\"})",
            "health": "ok",
            "evaluationTime": 0.012084086,
            "lastEvaluation": "2023-09-21T23:04:00.587517279Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.012099735,
        "lastEvaluation": "2023-09-21T23:04:00.587507424Z"
      },
      {
        "name": "thanos-query.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-thanos-querier.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ThanosQueryHttpRequestQueryErrorRateHigh",
            "query": "(sum(rate(http_requests_total{code=~\"5..\",handler=\"query\",job=\"thanos-querier\"}[5m])) / sum(rate(http_requests_total{handler=\"query\",job=\"thanos-querier\"}[5m]))) * 100 > 5",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} is failing to handle {{ $value | humanize }}% of \"query\" requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00100033,
            "lastEvaluation": "2023-09-21T23:03:46.838848511Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ThanosQueryHttpRequestQueryRangeErrorRateHigh",
            "query": "(sum(rate(http_requests_total{code=~\"5..\",handler=\"query_range\",job=\"thanos-querier\"}[5m])) / sum(rate(http_requests_total{handler=\"query_range\",job=\"thanos-querier\"}[5m]))) * 100 > 5",
            "duration": 300,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} is failing to handle {{ $value | humanize }}% of \"query_range\" requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000700838,
            "lastEvaluation": "2023-09-21T23:03:46.839852657Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ThanosQueryGrpcServerErrorRate",
            "query": "(sum by(job) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"thanos-querier\"}[5m])) / sum by(job) (rate(grpc_server_started_total{job=\"thanos-querier\"}[5m])) * 100 > 5)",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} is failing to handle {{ $value | humanize }}% of requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002785254,
            "lastEvaluation": "2023-09-21T23:03:46.840557118Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ThanosQueryGrpcClientErrorRate",
            "query": "(sum by(job) (rate(grpc_client_handled_total{grpc_code!=\"OK\",job=\"thanos-querier\"}[5m])) / sum by(job) (rate(grpc_client_started_total{job=\"thanos-querier\"}[5m]))) * 100 > 5",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} is failing to send {{ $value | humanize }}% of requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000703118,
            "lastEvaluation": "2023-09-21T23:03:46.843345777Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ThanosQueryHighDNSFailures",
            "query": "(sum by(job) (rate(thanos_querier_store_apis_dns_failures_total{job=\"thanos-querier\"}[5m])) / sum by(job) (rate(thanos_querier_store_apis_dns_lookups_total{job=\"thanos-querier\"}[5m]))) * 100 > 1",
            "duration": 900,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} have {{ $value | humanize }}% of failing DNS queries for store endpoints."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000626909,
            "lastEvaluation": "2023-09-21T23:03:46.844051222Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ThanosQueryInstantLatencyHigh",
            "query": "(histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{handler=\"query\",job=\"thanos-querier\"}[5m]))) > 40 and sum by(job) (rate(http_request_duration_seconds_bucket{handler=\"query\",job=\"thanos-querier\"}[5m])) > 0)",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} has a 99th percentile latency of {{ $value }} seconds for instant queries."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.004157738,
            "lastEvaluation": "2023-09-21T23:03:46.844681912Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ThanosQueryRangeLatencyHigh",
            "query": "(histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{handler=\"query_range\",job=\"thanos-querier\"}[5m]))) > 90 and sum by(job) (rate(http_request_duration_seconds_count{handler=\"query_range\",job=\"thanos-querier\"}[5m])) > 0)",
            "duration": 600,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "message": "Thanos Query {{$labels.job}} has a 99th percentile latency of {{ $value }} seconds for range queries."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001943408,
            "lastEvaluation": "2023-09-21T23:03:46.848843222Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.011953945,
        "lastEvaluation": "2023-09-21T23:03:46.838838759Z"
      },
      {
        "name": "multus-admission-controller-monitor-service.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-multus-prometheus-k8s-rules.yaml",
        "rules": [
          {
            "name": "cluster:network_attachment_definition_enabled_instance_up:max",
            "query": "max by(networks) (network_attachment_definition_enabled_instance_up)",
            "health": "ok",
            "evaluationTime": 0.000488016,
            "lastEvaluation": "2023-09-21T23:03:56.004313499Z",
            "type": "recording"
          },
          {
            "name": "cluster:network_attachment_definition_instances:max",
            "query": "max by(networks) (network_attachment_definition_instances)",
            "health": "ok",
            "evaluationTime": 0.000353282,
            "lastEvaluation": "2023-09-21T23:03:56.004803989Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000858364,
        "lastEvaluation": "2023-09-21T23:03:56.004304234Z"
      },
      {
        "name": "olm.failing_operators.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-operator-lifecycle-manager-olm-alert-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "FailingOperator",
            "query": "csv_abnormal{phase=\"Failed\"}",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Reason-{{ $labels.reason }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000216822,
            "lastEvaluation": "2023-09-21T23:03:54.527394241Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.000236436,
        "lastEvaluation": "2023-09-21T23:03:54.5273781Z"
      },
      {
        "name": "cluster-network-operator-sdn.rules",
        "file": "/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-sdn-networking-rules.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "NodeWithoutOVSPod",
            "query": "(kube_node_info unless on(node) topk by(node) (1, kube_pod_info{namespace=\"openshift-sdn\",pod=~\"ovs.*\"})) > 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "All nodes should be running an ovs pod, {{ $labels.node }} is not.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001212615,
            "lastEvaluation": "2023-09-21T23:03:48.005549983Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeWithoutSDNPod",
            "query": "(kube_node_info unless on(node) topk by(node) (1, kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn.*\"})) > 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "All nodes should be running an sdn pod, {{ $labels.node }} is not.\n"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000837255,
            "lastEvaluation": "2023-09-21T23:03:48.00676581Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeProxyApplySlow",
            "query": "histogram_quantile(0.95, kubeproxy_sync_proxy_rules_duration_seconds_bucket) * on(namespace, pod) group_right() topk by(namespace, pod) (1, kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn-[^-]*\"}) > 15",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "SDN pod {{ $labels.pod }} on node {{ $labels.node }} is taking too long, on average, to apply kubernetes service rules to iptables."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001717957,
            "lastEvaluation": "2023-09-21T23:03:48.007607009Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "ClusterProxyApplySlow",
            "query": "histogram_quantile(0.95, sum by(le) (rate(kubeproxy_sync_proxy_rules_duration_seconds_bucket[5m]))) > 10",
            "duration": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "The cluster is taking too long, on average, to apply kubernetes service rules to iptables."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001412142,
            "lastEvaluation": "2023-09-21T23:03:48.009328574Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeProxyApplyStale",
            "query": "(kubeproxy_sync_proxy_rules_last_queued_timestamp_seconds - kubeproxy_sync_proxy_rules_last_timestamp_seconds) * on(namespace, pod) group_right() topk by(namespace, pod) (1, kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn-[^-]*\"}) > 30",
            "duration": 300,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "SDN pod {{ $labels.pod }} on node {{ $labels.node }} has stale kubernetes service rules in iptables."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00052195,
            "lastEvaluation": "2023-09-21T23:03:48.010743326Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "SDNPodNotReady",
            "query": "kube_pod_status_ready{condition=\"true\",namespace=\"openshift-sdn\"} == 0",
            "duration": 600,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "message": "SDN pod {{ $labels.pod }} on node {{ $labels.node }} is not ready."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000312628,
            "lastEvaluation": "2023-09-21T23:03:48.01126695Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "evaluationTime": 0.006065839,
        "lastEvaluation": "2023-09-21T23:03:48.005518669Z"
      }
    ]
  }
}
